{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14fa1fb6-cc79-49bb-be0e-d90887cd3b3d",
   "metadata": {},
   "source": [
    "# LSTM and SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d931fff-f845-494a-a517-fc63aad609b6",
   "metadata": {},
   "source": [
    "Resources:\n",
    "- Textbook, Geron 2019: Aurelien Geron. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. Oâ€™Reilly Media, Sebastopol, CA, 3rd edition, 2023.\n",
    "- Interactive Video: [StatQuest - LSTM](https://www.youtube.com/watch?v=YCzL96nL7j0&t=358s&ab_channel=StatQuestwithJoshStarmer)\n",
    "\n",
    "Keras Documentation: \n",
    "- [Keras Code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent.py)\n",
    "- [SimpleRNN](https://keras.io/api/layers/recurrent_layers/simple_rnn/), [Dense](https://keras.io/api/layers/core_layers/dense/), [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/)\n",
    "\n",
    "Documentation on order of weights in LSTM: \n",
    "- [Stack Overflow Thread 1](https://stackoverflow.com/questions/68845790/gate-weights-order-for-lstm-layers-in-tensorflow)\n",
    "- [Stack Thread 2](https://stackoverflow.com/questions/46817085/keras-interpreting-the-output-of-get-weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38837308-838b-47df-ad93-1583b9f2457b",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848bb65-09b3-48c2-aa6d-98d285e58ea7",
   "metadata": {},
   "source": [
    "### Simple RNN Connections and Equations\n",
    "\n",
    "The Simple RNN cell modifies the standard neural network cell by adding a recurrent connection. NOTE: some people refer to a single unit as just a \"neuron\", and use the concept of a \"cell\" to refer to an entire computational layer of neurons. The output of the cell at the previous time step is stored as a \"hidden state\", and this state is combined in a linear combination with the input to the cell to create the cell output, which is then stored as the next hidden state. Let,\n",
    "- $h_t$: the hidden state, or the cell output, at time $t$, $h_t\\in \\mathbb R$\n",
    "- $X_t$: vector of external input at time $t$, $X_t \\in \\mathbb R^p$\n",
    "    - dimensions $p\\times 1$, where $p$ is the number of predictors, or the dimensionality of the features\n",
    "- $W_x$: weights associated with $X_t$, $W_x \\in \\mathbb R^p$\n",
    "    - dimensions $1\\times p$ (using this convention to simplify the multiplication notation, it's a dot product between $X_t$ and $W_x$)\n",
    "- $W_h$: weights associated with the hidden state $h_t$, aka the recurrent weight, $W_h\\in \\mathbb R$\n",
    "- $b$: bias, $b\\in \\mathbb R$\n",
    "\n",
    "The equation for the output of the cell at time $t$ is a function of a linear combination with activation function $\\phi: \\mathbb R\\to \\mathbb R$ (e.g. sigmoid, tanh, ReLU):\n",
    "\n",
    "$$\n",
    "h_t = \\phi(W_x\\cdot X_t + W_h\\cdot h_{t-1}+b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512283ce-edfa-4633-b9b0-a49a6f853847",
   "metadata": {},
   "source": [
    "## LSTM Connections and Equations\n",
    "\n",
    "The LSTM cell also has a hidden state $h_t$ that is the output of the cell and is stored as the short-term memory. The LSTM cell augments the Simple RNN cell by adding an additional state, called the \"cell state\" or the long-term memory. Further, the LSTM cell has several \"gates\" that control how the cell state is modified by new information. The hidden state $h_t$ and input $X_t$ are defined as before. The LSTM machinery uses 4 separate linear combos of $h_t$ and $X_t$, each with a different interpretation. All bias' are scalar values, and all weights associated with the hidden state are scalar, and all weights associated with the input are vectors of the same length as the input. \n",
    "\n",
    "**Forget Gate:** produces a value that is multiplied with the previous cell state. By default, this value is in $(0,1)$, so it is interpretable as the percentage of the long-term memory to retain. Let,\n",
    "- $W_x^{(f)}$: weight vector for input at forget gate\n",
    "- $W_h^{(f)}$: weight for hidden state at forget gate\n",
    "- $b^{(f)}$: bias at forget gate\n",
    "\n",
    "The forget gate by default uses the sigmoid activation function $\\sigma: \\mathbb R \\to (0,1)$, where $\\sigma(x) = \\frac{1}{1+e^{-x}} = \\frac{e^x}{1+e^x}$. The sigmoid maps the real number line to the open interval $(0,1)$, interpetable as probabilities or percentages. The percentage produced by the forget get is multplied by the cell state at the previous time step, so it is interpretable as a percentage of the old memory to retain. Let the forget gate be denoted $f_t$, and\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_x^{(f)} \\cdot X_t + W_h^{(f)}\\cdot h_t + b^{(f)}), \\quad \\in (0,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6e37e-bbcd-4a20-8bcc-881927ec11cc",
   "metadata": {},
   "source": [
    "**Candidate Memory:** produces a value interpretable as a proposed new contribution to the long-term cell state. This is often denoted as $g_t$, or sometimes $\\tilde c_t$. Let,\n",
    "- $W_x^{(g)}$: weight vector for input at candidate \n",
    "- $W_h^{(g)}$: weight for hidden state at candidate\n",
    "- $b^{(g)}$: bias at candidate\n",
    "\n",
    "By default, the candidate memory uses the hyperbolic tangent function $\\tanh: \\mathbb R\\to (-1,1)$, where $\\tanh(x) = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$. The tanh function maps the real numbers to the open interval $(-1,1)$. ReLU or any other activation function can be used in this stage, but using the unbounded ReLU can lead to stability issues during training that the LSTM was designed to remedy. Let the candidate be denoted $g_t$, and\n",
    "\n",
    "$$\n",
    "g_t = \\tanh(W_x^{(g)} \\cdot X_t + W_h^{(g)}\\cdot h_t + b^{(g)}), \\quad \\in (-1,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775a3d2-5fde-4a10-bff1-5dd6542e28e5",
   "metadata": {},
   "source": [
    "**Input Gate:** produces a value in $(0,1)$, interpretable as the percentager of the candidate memory to add to the long-term memory. Let,\n",
    "- $W_x^{(i)}$: weight vector for input at input gate \n",
    "- $W_h^{(i)}$: weight for hidden state at input gate\n",
    "- $b^{(i)}$: bias at input gate\n",
    "\n",
    "For the same reasons as the forget gate, the input gate typically utilizes sigmoid activation. Let the input gate be denotexd $i_t$, and\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(W_x^{(i)} \\cdot X_t + W_h^{(i)}\\cdot h_t + b^{(i)}), \\quad \\in (0,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca4ace-f62f-4616-913e-d090573f62b8",
   "metadata": {},
   "source": [
    "**Updating the Cell State**: the long-term memory is updated by applying the forget gate to retain a percentage of the old cell state, and then we add the candidate memory times the input gate\n",
    "\n",
    "$$\n",
    "c_t = f_t\\cdot c_{t-1} + i_t\\cdot g_t, \\quad \\in \\mathbb R\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c53a6-29f4-403c-9cd7-212e93b40915",
   "metadata": {},
   "source": [
    "**Output Gate:** produces a value that is used to construct the new hidden state. By default, this value is in $(0,1)$. This interpertation is more subtle, but values of the output gate near 0 means that the output $h_t$ retains almost nothing from the long-term memory. Values of the output gate near 1 means that $h_t$ is mostly a function of the cell state. Let,\n",
    "- $W_x^{(o)}$: weight vector for input at output gate \n",
    "- $W_h^{(o)}$: weight for hidden state at output gate\n",
    "- $b^{(o)}$: bias at output gate\n",
    "\n",
    "Again, by default the output gate utilizes sigmoid activation. Let the output gate be denoted $o_t$, and\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_x^{(o)} \\cdot X_t + W_h^{(o)}\\cdot h_t + b^{(o)}), \\quad \\in (0,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572718b-7862-41f5-b2c3-0b4f746ee4bf",
   "metadata": {},
   "source": [
    "**Updating the Hidden State:** the new hidden state of the LSTM, which is the output of the cell, is the product of the output gate and an activation modified cell state, by default tanh activation\n",
    "\n",
    "$$\n",
    "h_t = o_t \\cdot \\tanh(c_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d15042a-9bc5-471f-815b-24e73bccdc6e",
   "metadata": {},
   "source": [
    "## Connecting the LSTM and Simple RNN\n",
    "\n",
    "The simple RNN can be thought of as a special the LSTM when the forget gate is exactly zero, the input gate is exactly 1, and the output gate is exactly 1. The sigmoid function will never exactly produce 0 or 1. With sigmoid activation for the input/output/forget gates, an LSTM can produce outputs arbitrarily close to a simple RNN, but not exactly reproduce them. There are activation functions that could be used to produce exact 0's and 1's, like the \"hard sigmoid\", which is a piece-wise linear approximation to the sigmoid. \n",
    "\n",
    "Then, the candidate memory takes the place of the linear combination used in the simple RNN so that it becomes the new long term memory $c_t$ if the input gate was 1. In the LSTM cell, another activation function is applied to $c_t$ to generate the new hidden state $h_t$. If this activation were linear, there would be no issue and the LSTM cell would reproduce the simple RNN with the setup described in this section. However, by default in both tensorlfow and pytorch, you cannot set the activation used in the candidate to a different function than the one used to generate the new $h_t$. So if you used `activation=tanh` in the LSTM cell, the tanh function would be applied twice to the output and you would not reproduce the same values. Further, $\\lim_{x\\to \\infty}\\tanh(\\tanh(x)) = \\tanh(1) < 1$. We will proceed with the example that the simple RNN and LSTM candidate and final output use linear activation to make the math and programming easier.\n",
    "\n",
    "To recreate the output of a simple RNN cell using an LSTM, we can set the weights to zeros in the forget, input, and output gates and use large biases that approximate 0 or 1 depending on the gate. Observe that,\n",
    "\n",
    "$$\n",
    "\\sigma(-10)\\approx 0.00004 \\qquad\\qquad \\tanh(5)\\approx 0.99991\n",
    "$$\n",
    "\n",
    "So, we can almost exactly reproduce the outputs of the simple RNN by copying the weights from the simple RNN cell into the candidate $g_t$ with ReLU activation, and setting other weights to\n",
    "\n",
    "| Weight | Description | Value to set |\n",
    "|--------|-------------|-------|\n",
    "|    $W_x^{(f)}$    |      Forget gate input weight       |    0   |\n",
    "|    $W_h^{(f)}$    |      Forget gate recurrent weight       |    0   |\n",
    "|    $b^{(f)}$    |      Forget gate bias       |    -10   |\n",
    "|    $W_x^{(f)}$    |      Input gate input weight       |    0   |\n",
    "|    $W_h^{(f)}$    |      Gate gate recurrent weight       |    0   |\n",
    "|    $b^{(f)}$    |      Input gate bias       |    10   |\n",
    "|    $W_x^{(f)}$    |      Ouput gate input weight       |    0   |\n",
    "|    $W_h^{(f)}$    |      Output gate recurrent weight       |    0   |\n",
    "|    $b^{(f)}$    |      Output gate bias       |    10   |\n",
    "\n",
    "However, at $x=10$, the derivative of the sigmoid function is $\\sigma(10)(1-\\sigma(10))\\approx 0.00005$. So this would lead to incredibly slow training updates for those weights if the model was used in gradient descent. The training might not work at all, since the gradients of the other weights in the candidate memory step would be much larger and would be learning quickly compared to those weights hit by the near-zero gradient. The example with weights equal to $\\pm$10 will be used to show how the simple RNN can be recreated with the LSTM. But setting the weights to lower values like 3-5 would still be close to the simple RNN, and if it were to be trained with gradient descent it would have a better chance of learning at a reasonable rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d4de1-d768-44a9-98e4-555bcdc764c9",
   "metadata": {},
   "source": [
    "## Coding Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac28ac-8b24-4ecf-b31b-e08fe8fcf625",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f3116e-f1bc-455c-9d70-d069b49a32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefa3ad7-3b3e-4c48-aaca-616e498c71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures=3\n",
    "batch_size=None\n",
    "timesteps=None\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ac444-e36d-45f6-8fce-803be85338e6",
   "metadata": {},
   "source": [
    "### Simple RNN\n",
    "\n",
    "Network archicture:\n",
    "- Input: number of weights equal to features\n",
    "- Simple RNN: 1 recurrent weight, 1 bias\n",
    "- Dense Output Neuron: 1 weight, 1 bias\n",
    "\n",
    "For 3 features, expect 7 trainable parameterss in summary below.\n",
    "\n",
    "Default initialization for layers from Keras use zeros for bias. For SimpleRNN, you get $\\pm 1$ for a single cell since $[1]^T[1] = [1][1]^T=I_1$. NOTE: orthogonal matrices, not orthogonal vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502466ee-6eae-4d74-a12f-0bad713fea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(batch_shape=(batch_size, timesteps, nfeatures))\n",
    "x = tf.keras.layers.SimpleRNN(1, return_sequences=True, activation=\"linear\")(inputs)\n",
    "outputs = tf.keras.layers.Dense(1)(x)\n",
    "rnn = tf.keras.Model(inputs, outputs)\n",
    "rnn.compile(loss = \"mean_squared_error\", optimizer=\"Adam\")\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27925752-e09a-4ca1-ac45-801168bdfc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rweights = rnn.get_weights()\n",
    "print(\"~\"*25)\n",
    "print(f\"Input Weights: \")\n",
    "print(f\"    Shape: {rweights[0].shape}\")\n",
    "print(f\"    Values: {rweights[0].T}\")\n",
    "print(\"~\"*25)\n",
    "print(f\"Recurrent Weight:\")\n",
    "print(f\"    Shape: {rweights[1].shape}\")\n",
    "print(f\"    Values: {rweights[1].T}\")\n",
    "print(\"~\"*25)\n",
    "print(f\"Recurrent Bias:\")\n",
    "print(f\"    Shape: {rweights[2].shape}\")\n",
    "print(f\"    Values: {rweights[2].T}\")\n",
    "print(\"~\"*25)\n",
    "print(f\"Dense Output Weight: {rweights[3]}\")\n",
    "print(f\"    Shape: {rweights[3].shape}\")\n",
    "print(f\"    Values: {rweights[3].T}\")\n",
    "print(\"~\"*25)\n",
    "print(f\"Dense Output Bias: {rweights[4]}\")\n",
    "print(f\"    Shape: {rweights[4].shape}\")\n",
    "print(f\"    Values: {rweights[4].T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38afd4f3-4a63-44ab-9c8e-cb8a122e68d1",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "Network architecture:\n",
    "- Input: number of weights equal to features, denote $p$ \n",
    "- Gates and candidate each have: 1 bias + 1 recurrent weight + $p$ input weights = (1+1+$p$) parameters\n",
    "- Output neuron: 1 weight and 1 bias\n",
    "\n",
    "So for $p=3$ features, it has a number of parameters equal to $4\\cdot(1+1+3) = 20$, for a grand network total of $22$ with the output neuron parameters.\n",
    "\n",
    "Default initializers are zeros for the bias, but adding 1 to the bias in the forget gate. See the argument `unit_forget_bias` in the Keras documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087cba8b-9994-43b8-bc11-dad433a69277",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(batch_shape=(batch_size, timesteps, nfeatures))\n",
    "x = tf.keras.layers.LSTM(1, return_sequences=True, activation=\"linear\")(inputs)\n",
    "outputs = tf.keras.layers.Dense(1)(x)\n",
    "lstm = tf.keras.Model(inputs, outputs)\n",
    "lstm.compile(loss = \"mean_squared_error\", optimizer=\"Adam\")\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621cc08-a769-40de-b0df-2369135da4f4",
   "metadata": {},
   "source": [
    "## Set LSTM Weights to reproduce RNN\n",
    "\n",
    "Simple RNN is a special case of the LSTM. Using the initialized weights in the simple RNN, we set the weights for the LSTM in such a way that they generate the same output.\n",
    "\n",
    "The LSTM weights are a list of 3 arrays: input weights, recurrent weights, and biases. Within each array, based on stackoverflow thread, the order is `i, f, c, o` which stands for input gate, forget gate, cell gate and output gate respectively. This applies to the elements within a list of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9753fa-5d9a-4c6c-bf7e-b4b238797dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lweights = lstm.get_weights()\n",
    "print(\"~\"*25)\n",
    "print(f\"Input Weights: \")\n",
    "print(f\"    Shape: {lweights[0].shape}\")\n",
    "print(f\"    Values: {lweights[0]}\")\n",
    "print(\"~\"*25)\n",
    "print(f\"Recurrent Weight:\")\n",
    "print(f\"    Shape: {lweights[1].shape}\")\n",
    "print(f\"    Values: {lweights[1]}\")\n",
    "print(\"~\"*25)\n",
    "print(f\"Recurrent Bias:\")\n",
    "print(f\"    Shape: {lweights[2].shape}\")\n",
    "print(f\"    Values: {lweights[2].T}\")\n",
    "print(\"~\"*25)\n",
    "print(f\"Dense Output Weight: {lweights[3]}\")\n",
    "print(f\"    Shape: {lweights[3].shape}\")\n",
    "print(f\"    Values: {lweights[3].T}\")\n",
    "print(\"~\"*25)\n",
    "print(f\"Dense Output Bias: {lweights[4]}\")\n",
    "print(f\"    Shape: {lweights[4].shape}\")\n",
    "print(f\"    Values: {lweights[4].T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13943531-a61e-43f8-af12-873e84b9841e",
   "metadata": {},
   "source": [
    "We now set the weights of the LSTM to match the Simple RNN. See table above for detailed description.\n",
    "- Set weights for input, output, and forget gates to zeros.\n",
    "- Set biases for input, output, and forget gates to large values to get the sigmoid to approximate 0 or 1\n",
    "- Set the weights and biases for the candidate to match the simple RNN weights\n",
    "- Match weight and bias for output neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f713deee-35df-490b-b1a6-2071def383a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Input Weights\n",
    "lweights[0] = np.array([\n",
    "    [0,0, rweights[0][0][0], 0], # Feature 1\n",
    "    [0,0, rweights[0][1][0], 0], # Feature 2\n",
    "    [0,0, rweights[0][2][0], 0]  # Feature 3\n",
    "])\n",
    "\n",
    "# Set Recurrent Weights\n",
    "lweights[1] = np.array([[\n",
    "    0,                 # Input gate\n",
    "    0,                 # Forget gate\n",
    "    rweights[1][0][0], # Candidate, set to reccurent weight from SimpleRNN\n",
    "    0,                 # Output gate\n",
    "]])\n",
    "\n",
    "# Set Biases\n",
    "lweights[2] = np.array([\n",
    "    10,                # Input gate bias, set to approx 1\n",
    "    -10,               # Forget gate bias, set to approx 0\n",
    "    rweights[2][0],    # Candidate bias, set to bias from SimpleRNN\n",
    "    10,                # Output gate bias, set to approx 1\n",
    "])\n",
    "\n",
    "# Set output neuron weights\n",
    "lweights[3] = rweights[3]\n",
    "lweights[4] = rweights[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed973d91-f6b9-4e22-a079-cac83a5e1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.set_weights(lweights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b15c277-79e4-41ee-81f8-6de516fb281e",
   "metadata": {},
   "source": [
    "### Compare Predictions\n",
    "\n",
    "Check that predictions match for a few scenarios:\n",
    "- randomly generated normally distributed data\n",
    "- constant data for a few constant values for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0e939-9475-467e-b990-f3099014ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "nbatch=10\n",
    "nt = 10\n",
    "X = np.random.randn(nbatch, nt, nfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b255f-f26a-4b1a-9ff4-817e6210f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = rnn.predict(X)\n",
    "p2 = lstm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802ac8f-ba49-4e7b-b6b9-41ca2c3dedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max Difference between SimpleRNN and LSTM for {len(p1.flatten())} Standard Normal Inputs\")\n",
    "print(np.max(np.abs(p1-p2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e944976-205d-4c6a-bab6-0aaec82e4c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p1.flatten(), label=\"SimpleRNN Predictions\")\n",
    "plt.plot(p2.flatten(), linestyle=':', label=\"LSTM Predictions\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.title(f\"Predicted values for {len(p1.flatten())} Standard Normal Inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b089c1b8-4d1f-45ed-babe-6998a46ec878",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "X = np.zeros((nbatch, nt, nfeatures))\n",
    "constants = [-2, -1, 0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68069150-62a9-49da-95a6-5a5b15579848",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in constants:\n",
    "    print(\"~\"*50)\n",
    "    print(f\"Constant = {i}\")\n",
    "    p1 = rnn.predict(X+i)\n",
    "    p2 = lstm.predict(X+i)\n",
    "    print(f\"Max Difference between SimpleRNN and LSTM for {len(p1.flatten())} Standard Normal Inputs\")\n",
    "    print(np.max(np.abs(p1-p2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c383b17-6382-4606-bc9c-61bdc547742d",
   "metadata": {},
   "source": [
    "## Physics-Initiated RNN\n",
    "\n",
    "Using the scheme above, we recreate the physics initiated RNN in an LSTM cell.\n",
    "\n",
    "The discretized solution to the simplified version of the timelag ODE is:\n",
    "\n",
    "$$\n",
    "m_t = (1-e^{(-1/T)}) E + e^{(-1/T)} m_{t-1}\n",
    "$$\n",
    "\n",
    "For equilibrium moisture content $E$ and timelag constant $T$. Denote the above constants $W_x$ and $W_h$ and let $b=0$, so \n",
    "\n",
    "$$\n",
    "m_t = W_x E + W_h m_{t-1} + b\n",
    "$$\n",
    "\n",
    "These values can be used to set the weights and biases in a simple RNN to exactly reproduce the solution to the ODE. The dense output neuron is set to weight 1 and bias 0, note this isn't needed in the 1 neuron case but we keep it here to extend to when we stack multiple RNN cells to improve accuracy. Then, we will demonstrate applying it to the LSTM to approximate the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e3304-e53a-4aee-b556-cbd308006818",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures = 1 # Only Equilibrium MC\n",
    "T = 10 # 10hr FMC \n",
    "Wx = np.exp(-1/T)\n",
    "Wh = (1-Wx)\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5114d-8db3-499b-85f5-f274bba611f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple RNN and LSTM with only 1 input\n",
    "batch_size=None\n",
    "timesteps=None\n",
    "nfeatures=1\n",
    "\n",
    "## Simple RNN\n",
    "inputs = tf.keras.Input(batch_shape=(batch_size, timesteps, nfeatures))\n",
    "x = tf.keras.layers.SimpleRNN(1, return_sequences=True, activation=\"linear\")(inputs)\n",
    "outputs = tf.keras.layers.Dense(1)(x)\n",
    "rnn = tf.keras.Model(inputs, outputs)\n",
    "rnn.compile(loss = \"mean_squared_error\", optimizer=\"Adam\")\n",
    "## LSTM\n",
    "inputs = tf.keras.Input(batch_shape=(batch_size, timesteps, nfeatures))\n",
    "x = tf.keras.layers.LSTM(1, return_sequences=True, activation=\"linear\")(inputs)\n",
    "outputs = tf.keras.layers.Dense(1)(x)\n",
    "lstm = tf.keras.Model(inputs, outputs)\n",
    "lstm.compile(loss = \"mean_squared_error\", optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a22f2c-2f54-4cfd-ae0e-878201fe7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set simple RNN weights \n",
    "rweights = rnn.get_weights()\n",
    "\n",
    "rweights[0] = np.array([[Wx]]) # Input\n",
    "rweights[1] = np.array([[Wh]]) # Recurrent connection\n",
    "rweights[2] = np.array([0])    # RNN Cell Bias\n",
    "rweights[3] = np.array([[1]])  # Dense Output Activation\n",
    "rweights[4] = np.array([0])  # Dense Output Bias\n",
    "\n",
    "rnn.set_weights(rweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70be893-e5d8-4cc5-8a06-1f5697faa1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Equilibrium moisture content under a few scenarios\n",
    "    # Constant 5, 10, 20, 50 %\n",
    "    # Sine waves with 24 hour period, varying intensities\n",
    "nbatch = 1\n",
    "nt = 24\n",
    "X0 = np.zeros((nbatch, nt, 1))\n",
    "constants = [5, 10, 20, 50]\n",
    "Xc = np.vstack([X0 + c for c in constants])\n",
    "rnn_preds = rnn.predict(Xc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de44b6-e13c-436f-8d2e-5ab5b59230d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsteps = np.arange(nt)\n",
    "\n",
    "plt.plot(tsteps, rnn_preds[constants.index(50)], label=\"EMC=50\")\n",
    "plt.plot(tsteps, rnn_preds[constants.index(20)], label=\"EMC=20\")\n",
    "plt.plot(tsteps, rnn_preds[constants.index(10)], label=\"EMC=10\")\n",
    "plt.plot(tsteps, rnn_preds[constants.index(5)], label=\"EMC=5\")\n",
    "\n",
    "plt.ylabel(\"EMC (%)\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Simple RNN with Physics-Initialized Weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e9a3f-e53b-43e6-a8df-515d44b80544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Sine Waves and Predict\n",
    "nt = 24*3\n",
    "mean_val = 10\n",
    "amplitude = 5\n",
    "period_hours = 24\n",
    "\n",
    "timesteps = np.arange(nt)\n",
    "wave = amplitude * np.sin(2 * np.pi/period_hours * timesteps) + mean_val\n",
    "Xp = wave.reshape(1, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951e59b-1d77-42f0-a34d-4b4e3a8f6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = rnn.predict(Xp)\n",
    "\n",
    "plt.plot(timesteps, Xp.squeeze(), label=\"EMC\")\n",
    "plt.plot(timesteps, p1.squeeze(), label=\"Predicted FMC\")\n",
    "\n",
    "plt.ylabel(\"EMC (%)\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Simple RNN with Physics-Initialized Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872fa06-a94c-4692-b505-3bf975f929f8",
   "metadata": {},
   "source": [
    "## Physics Initiated LSTM\n",
    "\n",
    "Set weights appropriately.\n",
    "\n",
    "Use same data as simulated above, see if we can reproduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88434eba-8939-4679-9805-8136a92b09f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lweights = lstm.get_weights()\n",
    "\n",
    "# Set Input Weights\n",
    "lweights[0] = np.array([\n",
    "    [0,0, rweights[0][0][0], 0], # EMC weights\n",
    "])\n",
    "\n",
    "# Set Recurrent Weights\n",
    "lweights[1] = np.array([[\n",
    "    0,                 # Input gate\n",
    "    0,                 # Forget gate\n",
    "    rweights[1][0][0], # Candidate, set to reccurent weight from SimpleRNN\n",
    "    0,                 # Output gate\n",
    "]])\n",
    "\n",
    "# Set Biases\n",
    "lweights[2] = np.array([\n",
    "    10,                # Input gate bias, set to approx 1\n",
    "    -10,               # Forget gate bias, set to approx 0\n",
    "    rweights[2][0],    # Candidate bias, set to bias from SimpleRNN\n",
    "    10,                # Output gate bias, set to approx 1\n",
    "])\n",
    "\n",
    "# Set output neuron weights\n",
    "lweights[3] = rweights[3]\n",
    "lweights[4] = rweights[4]\n",
    "\n",
    "lstm.set_weights(lweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860117a4-aea0-4d64-ba1f-2e38316ea5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "lstm_preds = lstm.predict(Xc)\n",
    "p2 = lstm.predict(Xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360666f2-736e-4f59-97d5-2bf99fd8efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(lstm_preds - rnn_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e981e7f3-fadd-4ba6-9b7b-9aaeba671c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(p1 - p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8bba2-857b-42d0-80ce-b6a7fbd00784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
