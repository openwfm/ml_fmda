{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14fa1fb6-cc79-49bb-be0e-d90887cd3b3d",
   "metadata": {},
   "source": [
    "# LSTM and SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d931fff-f845-494a-a517-fc63aad609b6",
   "metadata": {},
   "source": [
    "Resources:\n",
    "- Textbook, Geron 2019: Aurelien Geron. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. Oâ€™Reilly Media, Sebastopol, CA, 3rd edition, 2023.\n",
    "- Interactive Video: [StatQuest - LSTM](https://www.youtube.com/watch?v=YCzL96nL7j0&t=358s&ab_channel=StatQuestwithJoshStarmer)\n",
    "\n",
    "Keras Documentation: \n",
    "- [Keras Code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent.py)\n",
    "- [SimpleRNN](https://keras.io/api/layers/recurrent_layers/simple_rnn/), [Dense](https://keras.io/api/layers/core_layers/dense/), [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/)\n",
    "\n",
    "Documentation on order of weights in LSTM: \n",
    "- [Stack Overflow Thread 1](https://stackoverflow.com/questions/68845790/gate-weights-order-for-lstm-layers-in-tensorflow)\n",
    "- [Stack Thread 2](https://stackoverflow.com/questions/46817085/keras-interpreting-the-output-of-get-weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38837308-838b-47df-ad93-1583b9f2457b",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848bb65-09b3-48c2-aa6d-98d285e58ea7",
   "metadata": {},
   "source": [
    "### Simple RNN Connections and Equations\n",
    "\n",
    "The Simple RNN cell modifies the standard neural network cell by adding a recurrent connection. NOTE: some people refer to a single unit as just a \"neuron\", and use the concept of a \"cell\" to refer to an entire computational layer of neurons. The output of the cell at the previous time step is stored as a \"hidden state\", and this state is combined in a linear combination with the input to the cell to create the cell output, which is then stored as the next hidden state. Let,\n",
    "- $h_t$: the hidden state, or the cell output, at time $t$, $h_t\\in \\mathbb R$\n",
    "- $X_t$: vector of external input at time $t$, $X_t \\in \\mathbb R^p$\n",
    "    - dimensions $p\\times 1$, where $p$ is the number of predictors, or the dimensionality of the features\n",
    "- $W_x$: weights associated with $X_t$, $W_x \\in \\mathbb R^p$\n",
    "    - dimensions $1\\times p$ (using this convention to simplify the multiplication notation, it's a dot product between $X_t$ and $W_x$)\n",
    "- $W_h$: weights associated with the hidden state $h_t$, aka the recurrent weight, $W_h\\in \\mathbb R$\n",
    "- $b$: bias, $b\\in \\mathbb R$\n",
    "\n",
    "The equation for the output of the cell at time $t$ is a function of a linear combination with activation function $\\phi: \\mathbb R\\to \\mathbb R$ (e.g. sigmoid, tanh, ReLU):\n",
    "\n",
    "$$\n",
    "h_t = \\phi(W_x\\cdot X_t + W_h\\cdot h_{t-1}+b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512283ce-edfa-4633-b9b0-a49a6f853847",
   "metadata": {},
   "source": [
    "## LSTM Connections and Equations\n",
    "\n",
    "The LSTM cell also has a hidden state $h_t$ that is the output of the cell and is stored as the short-term memory. The LSTM cell augments the Simple RNN cell by adding an additional state, called the \"cell state\" or the long-term memory. Further, the LSTM cell has several \"gates\" that control how the cell state is modified by new information. The hidden state $h_t$ and input $X_t$ are defined as before. The LSTM machinery uses 4 separate linear combos of $h_t$ and $X_t$, each with a different interpretation. All bias' are scalar values, and all weights associated with the hidden state are scalar, and all weights associated with the input are vectors of the same length as the input. \n",
    "\n",
    "**Forget Gate:** produces a value that is multiplied with the previous cell state. By default, this value is in $(0,1)$, so it is interpretable as the percentage of the long-term memory to retain. Let,\n",
    "- $W_x^{(f)}$: weight vector for input at forget gate\n",
    "- $W_h^{(f)}$: weight for hidden state at forget gate\n",
    "- $b^{(f)}$: bias at forget gate\n",
    "\n",
    "The forget gate by default uses the sigmoid activation function $\\sigma: \\mathbb R \\to (0,1)$, where $\\sigma(x) = \\frac{1}{1+e^{-x}} = \\frac{e^x}{1+e^x}$. The sigmoid maps the real number line to the open interval $(0,1)$, interpetable as probabilities or percentages. The percentage produced by the forget get is multplied by the cell state at the previous time step, so it is interpretable as a percentage of the old memory to retain. Let the forget gate be denoted $f_t$, and\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_x^{(f)} \\cdot X_t + W_h^{(f)}\\cdot h_t + b^{(f)}), \\quad \\in (0,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6e37e-bbcd-4a20-8bcc-881927ec11cc",
   "metadata": {},
   "source": [
    "**Candidate Memory:** produces a value interpretable as a proposed new contribution to the long-term cell state. This is often denoted as $g_t$, or sometimes $\\tilde c_t$. Let,\n",
    "- $W_x^{(g)}$: weight vector for input at candidate \n",
    "- $W_h^{(g)}$: weight for hidden state at candidate\n",
    "- $b^{(g)}$: bias at candidate\n",
    "\n",
    "By default, the candidate memory uses the hyperbolic tangent function $\\tanh: \\mathbb R\\to (-1,1)$, where $\\tanh(x) = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$. The tanh function maps the real numbers to the open interval $(-1,1)$. ReLU or any other activation function can be used in this stage, but using the unbounded ReLU can lead to stability issues during training that the LSTM was designed to remedy. Let the candidate be denoted $g_t$, and\n",
    "\n",
    "$$\n",
    "g_t = \\tanh(W_x^{(g)} \\cdot X_t + W_h^{(g)}\\cdot h_t + b^{(g)}), \\quad \\in (-1,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775a3d2-5fde-4a10-bff1-5dd6542e28e5",
   "metadata": {},
   "source": [
    "**Input Gate:** produces a value in $(0,1)$, interpretable as the percentager of the candidate memory to add to the long-term memory. Let,\n",
    "- $W_x^{(i)}$: weight vector for input at input gate \n",
    "- $W_h^{(i)}$: weight for hidden state at input gate\n",
    "- $b^{(i)}$: bias at input gate\n",
    "\n",
    "For the same reasons as the forget gate, the input gate typically utilizes sigmoid activation. Let the input gate be denotexd $i_t$, and\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(W_x^{(i)} \\cdot X_t + W_h^{(i)}\\cdot h_t + b^{(i)}), \\quad \\in (0,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca4ace-f62f-4616-913e-d090573f62b8",
   "metadata": {},
   "source": [
    "**Updating the Cell State**: the long-term memory is updated by applying the forget gate to retain a percentage of the old cell state, and then we add the candidate memory times the input gate\n",
    "\n",
    "$$\n",
    "c_t = f_t\\cdot c_{t-1} + i_t\\cdot g_t, \\quad \\in \\mathbb R\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c53a6-29f4-403c-9cd7-212e93b40915",
   "metadata": {},
   "source": [
    "**Output Gate:** produces a value that is used to construct the new hidden state. By default, this value is in $(0,1)$. This interpertation is more subtle, but values of the output gate near 0 means that the output $h_t$ retains almost nothing from the long-term memory. Values of the output gate near 1 means that $h_t$ is mostly a function of the cell state. Let,\n",
    "- $W_x^{(o)}$: weight vector for input at output gate \n",
    "- $W_h^{(o)}$: weight for hidden state at output gate\n",
    "- $b^{(o)}$: bias at output gate\n",
    "\n",
    "Again, by default the output gate utilizes sigmoid activation. Let the output gate be denoted $o_t$, and\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_x^{(o)} \\cdot X_t + W_h^{(o)}\\cdot h_t + b^{(o)}), \\quad \\in (0,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572718b-7862-41f5-b2c3-0b4f746ee4bf",
   "metadata": {},
   "source": [
    "**Updating the Hidden State:** the new hidden state of the LSTM, which is the output of the cell, is the product of the output gate and an activation modified cell state, by default tanh activation\n",
    "\n",
    "$$\n",
    "h_t = o_t \\cdot \\tanh(c_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d15042a-9bc5-471f-815b-24e73bccdc6e",
   "metadata": {},
   "source": [
    "## Connecting the LSTM and Simple RNN\n",
    "\n",
    "The simple RNN can be thought of as a special the LSTM when the forget gate is exactly zero, the input gate is exactly 1, and the output gate is exactly 1. The sigmoid function will never exactly produce 0 or 1. With the standard activation functions, an LSTM can produce outputs arbitrarily close to a simple RNN, but not exactly reproduce them. Then, the candidate memory takes the place of the linear combination used in the simple RNN so that it becomes the new long term memory $c_t$ if the input gate was 1. In the LSTM cell, another activation function is applied to $c_t$ to generate the new hidden state $h_t$. If this activation were linear, there would be no issue and the LSTM cell would reproduce the simple RNN with the setup described in this section. However, by default in both tensorlfow and pytorch, you cannot set the activation used in the candidate to a different function than the one used to generate the new $h_t$. So if you used `activation=tanh` in the LSTM cell, the tanh function would be applied twice to the output and you would not reproduce the same values. Further, $\\lim_{x\\to \\infty}\\tanh(\\tanh(x)) = \\tanh(1) < 1$. We will proceed with the example that the simple RNN and LSTM candidate and final output use linear activation to make the math and programming easier.\n",
    "\n",
    "To recreate the output of a simple RNN cell using an LSTM, we can set the weights to zeros in the forget, input, and output gates and use large biases that approximate 0 or 1 depending on the gate. Observe that,\n",
    "\n",
    "$$\n",
    "\\sigma(-10)\\approx 0.00004 \\qquad\\qquad \\tanh(5)\\approx 0.99991\n",
    "$$\n",
    "\n",
    "So, we can almost exactly reproduce the outputs of the simple RNN by copying the weights from the simple RNN cell into the candidate $g_t$ with ReLU activation, and setting other weights to\n",
    "\n",
    "| Weight | Description | Value to set |\n",
    "|--------|-------------|-------|\n",
    "|    $W_x^{(f)}$    |      Forget gate input weight       |    0   |\n",
    "|    $W_h^{(f)}$    |      Forget gate recurrent weight       |    0   |\n",
    "|    $b^{(f)}$    |      Forget gate bias       |    -10   |\n",
    "|    $W_x^{(f)}$    |      Input gate input weight       |    0   |\n",
    "|    $W_h^{(f)}$    |      Gate gate recurrent weight       |    0   |\n",
    "|    $b^{(f)}$    |      Input gate bias       |    10   |\n",
    "|    $W_x^{(f)}$    |      Ouput gate input weight       |    0   |\n",
    "|    $W_h^{(f)}$    |      Output gate recurrent weight       |    0   |\n",
    "|    $b^{(f)}$    |      Output gate bias       |    10   |\n",
    "\n",
    "However, at $x=10$, the derivative of the sigmoid function is $\\sigma(10)(1-\\sigma(10))\\approx 0.00005$. So this would lead to incredibly slow training updates for those weights if the model was used in gradient descent. The training might not work at all, since the gradients of the other weights in the candidate memory step would be much larger and would be learning quickly compared to those weights hit by the near-zero gradient. The example with weights equal to $\\pm$10 will be used to show how the simple RNN can be recreated with the LSTM. But setting the weights to lower values like 3-5 would still be close to the simple RNN, and if it were to be trained with gradient descent it would have a better chance of learning at a reasonable rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d4de1-d768-44a9-98e4-555bcdc764c9",
   "metadata": {},
   "source": [
    "## Coding Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac28ac-8b24-4ecf-b31b-e08fe8fcf625",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f3116e-f1bc-455c-9d70-d069b49a32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefa3ad7-3b3e-4c48-aaca-616e498c71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures=3\n",
    "batch_size=None\n",
    "timesteps=None\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ac444-e36d-45f6-8fce-803be85338e6",
   "metadata": {},
   "source": [
    "### Simple RNN\n",
    "\n",
    "Network archicture:\n",
    "- Input: number of weights equal to features\n",
    "- Simple RNN: 1 recurrent weight, 1 bias\n",
    "- Dense Output Neuron: 1 weight, 1 bias\n",
    "\n",
    "For 3 features, expect 7 trainable parameterss in summary below.\n",
    "\n",
    "Default initialization for layers from Keras use zeros for bias. For SimpleRNN, you get $\\pm 1$ for a single cell since $[1]^T[1] = [1][1]^T=I_1$. NOTE: orthogonal matrices, not orthogonal vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502466ee-6eae-4d74-a12f-0bad713fea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(batch_shape=(batch_size, timesteps, nfeatures))\n",
    "x = tf.keras.layers.SimpleRNN(1, return_sequences=True, activation=\"linear\")(inputs)\n",
    "outputs = tf.keras.layers.Dense(1)(x)\n",
    "rnn = tf.keras.Model(inputs, outputs, name = \"Return_Sequences_True\")\n",
    "rnn.compile(loss = \"mean_squared_error\", optimizer=\"Adam\")\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27925752-e09a-4ca1-ac45-801168bdfc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rweights = rnn.get_weights()\n",
    "print(f\"Input Weights: {rweights[0]}\")\n",
    "print(f\"Recurrent Weight: {rweights[1]}\")\n",
    "print(f\"Recurrent Bias: {rweights[2]}\")\n",
    "print(f\"Dense Output Weight: {rweights[3]}\")\n",
    "print(f\"Dense Output Bias: {rweights[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38afd4f3-4a63-44ab-9c8e-cb8a122e68d1",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "Network architecture:\n",
    "- Input: number of weights equal to features, denote $p$ \n",
    "- Gates and candidate each have: 1 bias + 1 recurrent weight + $p$ input weights = (1+1+$p$) parameters\n",
    "- Output neuron: 1 weight and 1 bias\n",
    "\n",
    "So for $p=3$ features, it has a number of parameters equal to $4\\cdot(1+1+3) = 20$, for a grand network total of $22$ with the output neuron parameters.\n",
    "\n",
    "Default initializers are zeros for the bias, but adding 1 to the bias in the forget gate. See the argument `unit_forget_bias` in the Keras documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087cba8b-9994-43b8-bc11-dad433a69277",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(batch_shape=(batch_size, timesteps, nfeatures))\n",
    "x = tf.keras.layers.LSTM(1, return_sequences=True, activation=\"linear\")(inputs)\n",
    "outputs = tf.keras.layers.Dense(1)(x)\n",
    "lstm = tf.keras.Model(inputs, outputs, name = \"Return_Sequences_True\")\n",
    "lstm.compile(loss = \"mean_squared_error\", optimizer=\"Adam\")\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621cc08-a769-40de-b0df-2369135da4f4",
   "metadata": {},
   "source": [
    "## Set LSTM Weights to reproduce RNN\n",
    "\n",
    "Simple RNN is a special case of the LSTM. Using the initialized weights in the simple RNN, we set the weights for the LSTM in such a way that they generate the same output.\n",
    "\n",
    "From stackoverflow thread, in `get_weights` the order is `i, f, c, o` which stands for input gate, forget gate, cell gate and output gate respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a027db-e92d-4dcc-bf89-5ac0fe041b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "lweights = lstm.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21e048-5ab3-456d-b638-c2885a3a0a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer = lstm.get_layer(\"lstm_5\")\n",
    "W, U, b = lstm_layer.get_weights()\n",
    "units = W.shape[1] // 4  # number of LSTM units\n",
    "\n",
    "# Input-to-gates weights\n",
    "W_i = W[:, :units]\n",
    "W_f = W[:, units:2*units]\n",
    "W_c = W[:, 2*units:3*units]\n",
    "W_o = W[:, 3*units:]\n",
    "\n",
    "# Recurrent-to-gates weights\n",
    "U_i = U[:, :units]\n",
    "U_f = U[:, units:2*units]\n",
    "U_c = U[:, 2*units:3*units]\n",
    "U_o = U[:, 3*units:]\n",
    "\n",
    "# Biases\n",
    "b_i = b[:units]\n",
    "b_f = b[units:2*units]\n",
    "b_c = b[2*units:3*units]\n",
    "b_o = b[3*units:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b15c277-79e4-41ee-81f8-6de516fb281e",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Randomly generate data, make them match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0e939-9475-467e-b990-f3099014ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "X = np.random.randn(5, 7, nfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b255f-f26a-4b1a-9ff4-817e6210f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = rnn.predict(X)\n",
    "p2 = lstm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802ac8f-ba49-4e7b-b6b9-41ca2c3dedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(p1 == p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e944976-205d-4c6a-bab6-0aaec82e4c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p1.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b089c1b8-4d1f-45ed-babe-6998a46ec878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68069150-62a9-49da-95a6-5a5b15579848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612af1ea-01d0-41b3-b0e3-c1b7f3ac001a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96464af-711a-4d10-82d0-ce5fabaef6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3b139-714c-4e2d-ba04-0f5b5773c190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e3304-e53a-4aee-b556-cbd308006818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3252ff-ec24-49a8-aea8-a8dceaf00681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
