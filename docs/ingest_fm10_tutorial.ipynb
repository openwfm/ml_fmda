{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f4cc18-d23f-48ea-84dc-b0328a65e8e5",
   "metadata": {},
   "source": [
    "# Data Ingest of 10-h Fuel Moisture Content\n",
    "\n",
    "This notebook demonstrates retrieval and filtering of 10-h dead FMC data from RAWS. Retrieval of 10-h FMC observations is done with the software package `SynopticPy` and a stash of RAWS data kept and maintained by the broader OpenWFM community. This notebook will demonstrate use of `Synopticpy` with a free token, so limits are placed on the number of sensor hours that can be requested. Only records within the past year are freely available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c537a5-1963-48ff-a502-30247065a07d",
   "metadata": {},
   "source": [
    "The main steps in the retrieval are:\n",
    "* Use `synoptic.Metadata` to determine the RAWS with FMC data in the given spatial domain and time frame\n",
    "* Use `synoptic.Timeseries` to retrieve all available data that may be relevant to FMC modeling. *NOTE:* the stations are selected so they must have FMC data, and then any other available variables are collected as a bonus. These data are used for exploratory purposes and quality control checks, but predictors for final modeling comes from HRRR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a6416-e53a-4ced-aa8b-eb9689909f08",
   "metadata": {},
   "source": [
    "For more info on python library API, see Brian Blaylock's `SynopticPy` [python package](https://github.com/blaylockbk/SynopticPy)\n",
    "\n",
    "For more info on available Synoptic RAWS variables, see [Synoptic Data](https://demos.synopticdata.com/variables/index.html) documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed774d-7bef-48e7-b9a6-8f4ba4e17d81",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b32fd-9d6b-4582-b724-4d2a094a6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import synoptic\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "sys.path.append('../src')\n",
    "from utils import Dict, time_intp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9721c0f9-709d-4bfe-91f1-914c5a034af1",
   "metadata": {},
   "source": [
    "A configuration file is used to control data ingest. Automated processes utilize the file `training_data_config.json` or `forecast_config.json`. In this tutorial, we will manually build a config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f188f45-4040-44f3-a4c7-344efdd4478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now(timezone.utc)\n",
    "end = end.replace(minute=0, second=0, microsecond=0)\n",
    "start = end - relativedelta(months=6)\n",
    "\n",
    "print(f\"Start Date of retrieval: {start}\")\n",
    "print(f\"End Date of retrieval: {end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680e49a-a2f3-4472-b5a2-5c9f79870c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Dict({\n",
    "    'start_time': start, # String as YYYY-MM-DD_HH:mm:ss OR datetime object\n",
    "    'end_time': end,\n",
    "    'bbox': [40, -105, 45, -100], # [min_lat, min_lon, max_lat, max_lon]\n",
    "    'raws_weather_vars': [\"air_temp\", \"relative_humidity\", \"precip_accum\", \"fuel_moisture\", \"wind_speed\", \"solar_radiation\", \"pressure\", \"soil_moisture\", \"soil_temp\", \"snow_depth\", \"snow_accum\", \"wind_direction\"],\n",
    "    'raws_static_vars': [\"stid\", \"latitude\", \"longitude\", \"elevation\", \"name\", \"state\", \"id\"]\n",
    "})\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19298030-f0c2-41dd-99ff-530eb6d2eaf8",
   "metadata": {},
   "source": [
    "## Stations MetaData\n",
    "\n",
    "*Note*: the bounding box format used in `wrfxpy` is `[min_lat, min_lon, max_lat, max_lon]`. But, the bounding box format used by Synoptic is `[min_lon, min_lat, max_lon, max_lat]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a20a4-dd0f-44d1-8670-ca914111f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = config.bbox\n",
    "bbox_reordered = [bbox[1], bbox[0], bbox[3], bbox[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f6579-4836-463a-b1e6-4ee282d6a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = synoptic.Metadata(\n",
    "    bbox=bbox_reordered,\n",
    "    vars=[\"fuel_moisture\"], # Note we only want to include stations with FMC. Other \"raws_vars\" are bonus\n",
    "    obrange=(start, end),\n",
    ").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148a09f-5277-4f64-a582-fa97087358b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbefa25-ac25-4164-adbc-187f9740b6b8",
   "metadata": {},
   "source": [
    "## Station Time Series\n",
    "\n",
    "We loop over the station IDs found in the previous step and retrieve all available data and then format and clean.\n",
    "\n",
    "*NOTE*: this process is not parallelized, as the same IP address is used for each request and parallization may result in issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0abc62d-f063-48f8-8677-209f7f33525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_mapping = {\n",
    "#     \"air_temp\":\"temp\", \n",
    "#     \"fuel_moisture\":\"fm\", \n",
    "#     \"relative_humidity\":\"rh\", \n",
    "#     \"precip_accum\":\"rain\",\n",
    "#     \"solar_radiation\":\"solar\", \n",
    "#     \"wind_speed\":\"wind\", \n",
    "#     \"precip_accum\":\"precip_accum\", \n",
    "#     \"soil_moisture\":\"soil_moisture\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e6e0e-57a8-4621-a904-7cad9a05912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_raws(df, static_vars, weather_vars):\n",
    "    # Given input dataframe (the output of synoptic.TimeSeries), return formatted dictionary\n",
    "    # Inputs:\n",
    "    # df: (dataframe)\n",
    "    # Returns: tuple of data, units   \n",
    "\n",
    "    assert \"fuel_moisture\" in df[\"variable\"], \"fuel_moisture not detected in input dictionary\"\n",
    "    units = {} # stores units for variables\n",
    "    \n",
    "    \n",
    "    for var in weather_vars:\n",
    "        if var in df['variable']:\n",
    "            df_temp = df.filter(df['variable'] == var)\n",
    "            unit = df_temp['units'].unique()\n",
    "            if len(unit) != 1:\n",
    "                raise ValueError(f\"Variable {var} has multiple values for units\")\n",
    "            units[var] = unit[0]\n",
    "    \n",
    "    dat = df.filter(pl.col(\"variable\").is_in(weather_vars))\n",
    "    dat = dat.pivot(\n",
    "        values=\"value\",\n",
    "        index=[\"date_time\"]+static_vars,\n",
    "        on=\"variable\"\n",
    "    )\n",
    "\n",
    "    print(f\"Found {dat.shape[0]} FMC records\")\n",
    "    \n",
    "    # Fix column units\n",
    "    if \"air_temp\" in dat.columns and units['air_temp'] == \"Celsius\":\n",
    "        print(\"Converting RAWS air temp from C to K\")\n",
    "        units['air_temp'] = \"Kelvin\"\n",
    "        dat = dat.with_columns(\n",
    "                (pl.col(\"air_temp\")+273.15).alias(\"air_temp\")\n",
    "            )\n",
    "        \n",
    "    if 'elevation' in static_vars: # convert ft to meters\n",
    "        print(\"Converting RAWS elevation from ft to meters\")\n",
    "        # loc['elevation'] = loc['elevation'] * 0.3048\n",
    "        dat = dat.with_columns(\n",
    "                (pl.col(\"elevation\") * 0.3048).alias(\"elevation\")\n",
    "            )\n",
    "        units['elevation'] = \"m\"    \n",
    "        \n",
    "        \n",
    "    return dat, units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3adf8-deef-4897-bda5-6376b205efa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_temp = synoptic.TimeSeries(\n",
    "        stid=\"HSYN1\",\n",
    "        start=start,\n",
    "        end=end,\n",
    "        vars=config.raws_weather_vars,\n",
    "        units = \"metric\"\n",
    "    ).df()\n",
    "\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe06f7-e828-40a7-b95c-01718d235055",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp[\"date_time\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681bb2e8-11c9-426c-8ba3-9655f4b906a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp[\"date_time\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51389d-8189-4385-a04e-e1d4430a43b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b5bf6-6d2c-4bfc-8c6a-c806cc21e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64d531-316c-46a8-8106-b6ffddadfbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat, units = format_raws(df_temp, static_vars = config.raws_static_vars, weather_vars = config.raws_weather_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd18139-9f5f-4022-8f1f-698c840999ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10cc831-f309-4d3d-af3a-20ca50b01358",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d8934-d09c-499f-bf6c-c741ebaf4336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_static(df, static_vars):\n",
    "    \"\"\"\n",
    "    Given dataframe of timeseries observations from RAWS station, get dictionary of static info, such as identifiers and physical attributes of station.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe with timeseries observations.\n",
    "        static_vars: List of column names to extract static information from.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary called \"loc\" containing the unique value for each column in static_vars.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If any column in static_vars has more than one unique value in the dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    loc = {}\n",
    "    for col in static_vars:\n",
    "        if col in df.columns:\n",
    "            unique_values = df[col].unique()\n",
    "            if len(unique_values) == 1:\n",
    "                loc[col] = unique_values[0]\n",
    "            else:\n",
    "                raise ValueError(f\"Column '{col}' has more than one unique value: {unique_values}\")\n",
    "        else:\n",
    "            raise KeyError(f\"Column '{col}' not found in the dataframe.\")\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb05f7b-24d7-4811-9b12-d16764d56561",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Attempting retrieval of RAWS from {start} to {end} within {bbox}\")\n",
    "print(\"~\"*75)\n",
    "\n",
    "raws_dict = {}\n",
    "\n",
    "for st in sts['stid']:\n",
    "    print(\"~\"*50)\n",
    "    print(f\"Attempting retrival of station {st}\")\n",
    "    df = synoptic.TimeSeries(\n",
    "        stid=st,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        vars=config.raws_weather_vars,\n",
    "        units = \"metric\"\n",
    "    ).df()\n",
    "    \n",
    "    dat, units = format_raws(df, static_vars = config.raws_static_vars, weather_vars = config.raws_weather_vars)\n",
    "    loc = get_static(dat, config.raws_static_vars)\n",
    "    raws_dict[st] = {\n",
    "        'RAWS': dat,\n",
    "        'units': units,\n",
    "        'loc': loc,\n",
    "        'misc': \"Data retrieved using `synoptic.TimeSeries` and formatted with custom functions within `ml_fmda` project.\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc95c3b-bf47-4d85-a3df-919d23a833d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed191a8-78fa-4cfe-92e0-1e005644494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = [*raws_dict.keys()][0]\n",
    "raws_dict[st].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa62b9-cd10-43a7-86c1-a2d901b78ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict[st]['loc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300f861-25f0-4476-aaba-4a68ae45b5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict[st]['units']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea92b6-4d7a-453d-adf0-46495e761f57",
   "metadata": {},
   "source": [
    "## Fix Time and Interpolate\n",
    "\n",
    "Synoptic may return RAWS data that has missing hours or is returned not exactly on the hour. The missing hours are simply absent in the return data, not marked by NaN. We fix that by filling in NaN for missing hours and interpolating to the exact hour. The resulting data should have regular hourly observations for every RAWS station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7fb2f-fcab-4322-8308-e5899437b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = pl.datetime_range(\n",
    "    start=start,\n",
    "    end=end,\n",
    "    interval=\"1h\",\n",
    "    time_zone = \"UTC\",\n",
    "    eager=True\n",
    ").alias(\"time\")\n",
    "# times = np.array([dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\") for dt in times.to_list()])\n",
    "times = np.array(times.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a02a35-01d7-47c5-8a30-305bed4dd379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_intp_df(df, target_times, static_cols, time_cols):\n",
    "    \"\"\"\n",
    "    Interp and ...\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Interpolating dataframe in time from {target_times.min()} to {target_times.max()}\")\n",
    "    print(f\"    Original Dataframe shape: {df.shape}\")\n",
    "\n",
    "    # Get raw datetime values as numpy array\n",
    "    time_raws = np.array(df[\"date_time\"].to_list())    \n",
    "\n",
    "    # Interpolate time dynamic columns only for columns that exist in the dataframe\n",
    "    weather_data = {\n",
    "        var: time_intp(\n",
    "            time_raws, \n",
    "            df[var].to_numpy(), \n",
    "            target_times\n",
    "        ) for var in time_cols if var in df.columns\n",
    "    }\n",
    "    # Create a Polars DataFrame from the interpolated results\n",
    "    weather_df = pl.DataFrame(weather_data)\n",
    "    weather_df = weather_df.with_columns(pl.Series(\"date_time\", target_times))\n",
    "\n",
    "    # Expand only for columns that exist in the dataframe\n",
    "    nrow = weather_df.shape[0]\n",
    "    static_data = {\n",
    "        var: np.repeat(df[var].to_numpy()[0], nrow)\n",
    "        for var in static_cols if var in df.columns\n",
    "    }\n",
    "    static_df = pl.DataFrame(static_data)  \n",
    "    \n",
    "    # Combine interpolated weather data and expanded static variables\n",
    "    result_df = pl.concat([weather_df, static_df], how=\"horizontal\")\n",
    "    result_df = result_df.select(df.columns) # reorder columns to match original\n",
    "\n",
    "    print(f\"    Interpolated DataFrame shape: {result_df.shape}\")\n",
    "    print(f\"        interpolated {result_df.shape[0] - df.shape[0]} time steps\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea61230-cc2b-4f96-b104-23e4bbc87ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = time_intp_df(raws_dict[\"BRLW4\"][\"RAWS\"], times, static_cols = config.raws_static_vars, time_cols = config.raws_weather_vars)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd8c30-2cf0-4d45-9fac-3d0c09d1afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for st in raws_dict:\n",
    "    print(\"~\"*75)\n",
    "    print(st)\n",
    "    raws_dict[st][\"RAWS\"] = time_intp_df(raws_dict[st][\"RAWS\"], times, static_cols = config.raws_static_vars, time_cols = config.raws_weather_vars)\n",
    "    raws_dict[st][\"times\"] = times\n",
    "    raws_dict[\"BRLW4\"][\"misc\"] += \" Interpolated data with numpy linear interpolation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f9c46-7787-4f90-86ce-413d18c923d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b2419-3f82-4aa4-8f6e-ea8397099bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee2425-a199-4514-82f9-45380ef9f473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd78676-da71-4e1e-aef0-a0f1104d3ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
