{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f4cc18-d23f-48ea-84dc-b0328a65e8e5",
   "metadata": {},
   "source": [
    "# Data Ingest of 10-h Fuel Moisture Content\n",
    "\n",
    "This notebook demonstrates retrieval and filtering of 10-h dead FMC data from RAWS. Retrieval of 10-h FMC observations is done with the software package `SynopticPy` and a stash of RAWS data kept and maintained by the broader OpenWFM community. This notebook will demonstrate use of `Synopticpy` with a free token, so limits are placed on the number of sensor hours that can be requested. Only records within the past year are freely available.\n",
    "\n",
    "The module `ingest/retrieve_raws_api.py` has an executable section and will be run from the command line within this project. Here, the functions are used individually to demonstrate their utility. \n",
    "\n",
    "Time frame and spatial domain for data ingest are controlled in automated processes in the configuration files `training_data_config.json` or the `forecast_config.json` files. We will manually enter time frame and spatial domain variables in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c537a5-1963-48ff-a502-30247065a07d",
   "metadata": {},
   "source": [
    "The main steps in the retrieval are:\n",
    "* Use `synoptic.Metadata` to determine the RAWS with FMC data in the given spatial domain and time frame\n",
    "* Use `synoptic.Timeseries` to retrieve all available data that may be relevant to FMC modeling. *NOTE:* the stations are selected so they must have FMC data, and then any other available variables are collected as a bonus. These data are used for exploratory purposes and quality control checks, but predictors for final modeling comes from HRRR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079a0743-ad31-4d16-8307-34cf84b5c28b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "For more info on python library API, see Brian Blaylock's `SynopticPy` [python package](https://github.com/blaylockbk/SynopticPy)\n",
    "\n",
    "For more info on available Synoptic RAWS variables, see [Synoptic Data](https://demos.synopticdata.com/variables/index.html) documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed774d-7bef-48e7-b9a6-8f4ba4e17d81",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b32fd-9d6b-4582-b724-4d2a094a6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timezone\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import synoptic\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "sys.path.append('../src')\n",
    "from utils import Dict, read_yml\n",
    "# from utils import Dict, time_intp\n",
    "import ingest.retrieve_raws_api as rfuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f188f45-4040-44f3-a4c7-344efdd4478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now(timezone.utc)\n",
    "end = end.replace(minute=0, second=0, microsecond=0)\n",
    "start = end - relativedelta(months=2)\n",
    "bbox = [40, -105, 45, -100] # subset of the rocky mountain GACC region\n",
    "\n",
    "\n",
    "print(f\"Start Date of retrieval: {start}\")\n",
    "print(f\"End Date of retrieval: {end}\")\n",
    "print(f\"Spatial Domain: {bbox}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19298030-f0c2-41dd-99ff-530eb6d2eaf8",
   "metadata": {},
   "source": [
    "## Stations MetaData\n",
    "\n",
    "*Note*: the bounding box format used in `wrfxpy` is `[min_lat, min_lon, max_lat, max_lon]`. But, the bounding box format used by Synoptic is `[min_lon, min_lat, max_lon, max_lat]`. The code will assume the `wrfxpy` format and convert internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a20a4-dd0f-44d1-8670-ca914111f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_reordered = [bbox[1], bbox[0], bbox[3], bbox[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b8d5ae-8dd6-4209-9abc-751518d91968",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_reordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f6579-4836-463a-b1e6-4ee282d6a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = synoptic.Metadata(\n",
    "    bbox=bbox_reordered,\n",
    "    vars=[\"fuel_moisture\"], # Note we only want to include stations with FMC. Other \"raws_vars\" are bonus\n",
    "    obrange=(start, end),\n",
    ").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d07e391-4166-46fd-a1f8-ab7a1e7945af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbefa25-ac25-4164-adbc-187f9740b6b8",
   "metadata": {},
   "source": [
    "## Station Time Series\n",
    "\n",
    "Timeseries of observations are drawn for a single RAWS using the `synopticpy` package. Then, the data are formatted by custom funcitons in the `retrieve_raws_api` module. We subtract one hour from the start time because most stations produce data some number of minutes after the requested time, so if you request data at 1:00 the API will return data after that time. Then the temporal interpolation procedure, described below, will be extrapolating out at end points. Shifting the start time by 1 hour accounts for this, but if the start time is longer than 1 year in the past the API will truncate to 1 year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3adf8-deef-4897-bda5-6376b205efa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather_vars = rfuncs.raws_vars_dict[\"raws_weather_vars\"]\n",
    "df_temp = synoptic.TimeSeries(\n",
    "        stid=\"HSYN1\",\n",
    "        start=start-relativedelta(hours=1),\n",
    "        end=end,\n",
    "        vars=weather_vars,\n",
    "        units = \"metric\"\n",
    "    ).df()\n",
    "\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64d531-316c-46a8-8106-b6ffddadfbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat, units = rfuncs.format_raws(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd18139-9f5f-4022-8f1f-698c840999ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10cc831-f309-4d3d-af3a-20ca50b01358",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a326d07-edf5-4285-a7c0-e81caf1774ed",
   "metadata": {},
   "source": [
    "We then loop over the station IDs found in the previous step and retrieve all available data and then rename and pivot from long to wide.\n",
    "\n",
    "*NOTE*: this process is not parallelized, as the same IP address is used for each request and parallization may result in issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb05f7b-24d7-4811-9b12-d16764d56561",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Attempting retrieval of RAWS from {start} to {end} within {bbox}\")\n",
    "print(\"~\"*75)\n",
    "\n",
    "raws_dict = {}\n",
    "\n",
    "for st in sts['stid']:\n",
    "    print(\"~\"*50)\n",
    "    print(f\"Attempting retrival of station {st}\")\n",
    "    df = synoptic.TimeSeries(\n",
    "        stid=st,\n",
    "        start=start-relativedelta(hours=1),\n",
    "        end=end,\n",
    "        vars=weather_vars,\n",
    "        units = \"metric\"\n",
    "    ).df()\n",
    "    \n",
    "    dat, units = rfuncs.format_raws(df)\n",
    "    loc = rfuncs.get_static(dat)\n",
    "    raws_dict[st] = {\n",
    "        'RAWS': dat,\n",
    "        'units': units,\n",
    "        'loc': loc,\n",
    "        'misc': \"Data retrieved using `synoptic.TimeSeries` and formatted with custom functions within `ml_fmda` project.\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc95c3b-bf47-4d85-a3df-919d23a833d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed191a8-78fa-4cfe-92e0-1e005644494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = [*raws_dict.keys()][0]\n",
    "raws_dict[st].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa62b9-cd10-43a7-86c1-a2d901b78ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict[st]['loc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300f861-25f0-4476-aaba-4a68ae45b5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict[st]['units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89613564-a630-4ceb-97c1-6a15d3361530",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict[st]['misc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea92b6-4d7a-453d-adf0-46495e761f57",
   "metadata": {},
   "source": [
    "## Fix Time and Interpolate\n",
    "\n",
    "Synoptic may return RAWS data that has missing hours or is returned not exactly on the hour. The missing hours are simply absent in the return data, not marked by NaN. We fix that by filling in NaN for missing hours and interpolating to the exact hour. The resulting data should have regular hourly observations for every RAWS station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7fb2f-fcab-4322-8308-e5899437b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = pl.datetime_range(\n",
    "    start=start,\n",
    "    end=end,\n",
    "    interval=\"1h\",\n",
    "    time_zone = \"UTC\",\n",
    "    eager=True\n",
    ").alias(\"time\")\n",
    "# times = np.array([dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\") for dt in times.to_list()])\n",
    "times = np.array(times.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea61230-cc2b-4f96-b104-23e4bbc87ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = rfuncs.time_intp_df(raws_dict[\"BRLW4\"][\"RAWS\"], times)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106051c-b6ae-45a2-aafe-aaf8445c6de7",
   "metadata": {},
   "source": [
    "We now loop over all stations and run temporal interpolation. We also convert to pandas for easier pickle write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd8c30-2cf0-4d45-9fac-3d0c09d1afb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Interpolating dataframe in time from {times.min()} to {times.max()}\")\n",
    "for st in raws_dict:\n",
    "    print(\"~\"*75)\n",
    "    print(st)\n",
    "    nsteps = raws_dict[st][\"RAWS\"].shape[0]\n",
    "    raws_dict[st][\"RAWS\"] = rfuncs.time_intp_df(raws_dict[st][\"RAWS\"], times)\n",
    "    raws_dict[st][\"RAWS\"] = pd.DataFrame(raws_dict[st][\"RAWS\"], columns = raws_dict[st][\"RAWS\"].columns)\n",
    "    raws_dict[st][\"times\"] = times\n",
    "    if raws_dict[st][\"RAWS\"].shape[0] != nsteps:\n",
    "        raws_dict[st][\"misc\"] += \" Interpolated data with numpy linear interpolation.\"\n",
    "        print(f\"    Original Dataframe time steps: {nsteps}\")\n",
    "        print(f\"    Interpolated DataFrame time steps: {raws_dict[st][\"RAWS\"].shape[0]}\")\n",
    "        print(f\"        interpolated {raws_dict[st][\"RAWS\"].shape[0] - nsteps} time steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e7000-cd5a-4835-994c-2597e513fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"../data/raws_test.pkl\", 'wb') as file:\n",
    "#     pickle.dump(raws_dict, file)\n",
    "# with open(\"../data/raws_test.pkl\", \"rb\") as f:\n",
    "#     dat = pickle.load(f)\n",
    "# print(dat.keys())\n",
    "# dat[\"BRLW4\"][\"RAWS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7c95f-e7fd-43af-9c64-80625a318310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2457bf-0688-4147-b50d-c1d6c7b83181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac512fe9-6309-45e1-bd85-86225123a167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
