{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f4cc18-d23f-48ea-84dc-b0328a65e8e5",
   "metadata": {},
   "source": [
    "# Data Ingest of RAWS 10-h Fuel Moisture Content\n",
    "\n",
    "This notebook demonstrates retrieval and filtering of 10-h dead FMC data from RAWS. \n",
    "- Realtime 10-h FMC observations are retrieved with `SynopticPy`\n",
    "- Old 10-h FMC observations are retrieved from a stash MesoDB maintained by Angel Farguell\n",
    "\n",
    "This notebook will demonstrate use of `Synopticpy` with a free token, so limits are placed on the number of sensor hours that can be requested. Only records within the past year are freely available. Time frame and spatial domain for data ingest are controlled in automated processes in the configuration files `training_data_config.json` or the `forecast_config.json` files. This notebook will demonstrate manual data retrieval with short time frames for illustration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c537a5-1963-48ff-a502-30247065a07d",
   "metadata": {},
   "source": [
    "User inputs for data retrieval are:\n",
    "- Start time\n",
    "- End time\n",
    "- Spatial bounding box (see rtma_cycler in wrfxpy for GACC bbox's)\n",
    "\n",
    "The main steps in the retrieval are:\n",
    "* Use `synoptic.Metadata` to determine the RAWS with FMC data in the given spatial domain and time frame\n",
    "* Get data from stash OR use `synoptic.Timeseries` to retrieve all available data that may be relevant to FMC modeling. *NOTE:* the stations are selected so they must have FMC data, and then any other available variables are collected as a bonus. These data are used for exploratory purposes and quality control checks, but predictors for final modeling comes from HRRR.\n",
    "* Format data and convert units.\n",
    "* Identify missing data and interpolate with linear interpolation from numpy so resulting data is regular 1 hour intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079a0743-ad31-4d16-8307-34cf84b5c28b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "For more info on python library API, see Brian Blaylock's `SynopticPy` [python package](https://github.com/blaylockbk/SynopticPy)\n",
    "\n",
    "For more info on available Synoptic RAWS variables, see [Synoptic Data](https://demos.synopticdata.com/variables/index.html) documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed774d-7bef-48e7-b9a6-8f4ba4e17d81",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b32fd-9d6b-4582-b724-4d2a094a6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from datetime import datetime, timezone\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import synoptic\n",
    "# import json\n",
    "# import sys\n",
    "import numpy as np\n",
    "# import polars as pl\n",
    "import pandas as pd\n",
    "sys.path.append('../src')\n",
    "from utils import Dict, read_yml, print_dict_summary, str2time, time_range, rename_dict\n",
    "import ingest.RAWS as rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0837e63-176c-4e95-a49c-4f1784f94a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = [40, -105, 45, -100] # subset of rocky mountain gacc\n",
    "start = str2time('2024-06-01T00:00:00Z')\n",
    "end = str2time('2024-06-01T05:00:00Z')\n",
    "\n",
    "raws_meta = Dict(read_yml(\"../etc/variable_metadata/raws_metadata.yaml\"))\n",
    "print_dict_summary(raws_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19298030-f0c2-41dd-99ff-530eb6d2eaf8",
   "metadata": {},
   "source": [
    "## Stations MetaData\n",
    "\n",
    "We use `SynopticPy` to get a list of all RAWS stations within the bounding box that have fuel moisture data availability in the given time period. The function `get_stations` wraps the `synoptic.Metadata` function to order the bounding box properly and retrieve stations with FMC sensors.\n",
    "\n",
    "*Note*: the bounding box format used in `wrfxpy` is `[min_lat, min_lon, max_lat, max_lon]`. But, the bounding box format used by Synoptic is `[min_lon, min_lat, max_lon, max_lat]`. The code will assume the `wrfxpy` format and convert internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f6579-4836-463a-b1e6-4ee282d6a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = rr.get_stations(bbox)\n",
    "\n",
    "print(sts[\"stid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbefa25-ac25-4164-adbc-187f9740b6b8",
   "metadata": {},
   "source": [
    "## API Weather Data Time Series\n",
    "\n",
    "Timeseries of observations are drawn for a single RAWS using the `synopticpy` package. Then, the data are formatted by custom funcitons in the `ingest.RAWS` module. \n",
    "\n",
    "We subtract one hour from the start time and add one hour to the end. This is because most stations produce data some number of minutes after the requested time, so if you request data at 1:00 the API will return data after that time. Then the temporal interpolation procedure, described below, will be extrapolating out at end points. Shifting the start time by 1 hour accounts for this, but if the start time is longer than 1 year in the past the API will truncate to 1 year. The module has a metadata file with a list of all RAWS weather variables relevant to FMC modeling. \n",
    "\n",
    "The `raws_metadata` file has a list of \"static\" variables that are unchanging in time. In the data returned by SynopticPy, these variables are arranged differently than the time-dynamic weather sensor variables, which are also listed in the metadata file. Module functions combine these two types of variables into one tabular dataframe.\n",
    "\n",
    "The data is returned in \"long\" format, where each weather variable has its own row. We restructure the data into \"wide\" format with the module function `format_raws` so that a single row corresponds to one time, and the columns correspond to different data variables. Additionally, this function converts units and returns a dictionary of all units for the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3adf8-deef-4897-bda5-6376b205efa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather_vars = rr.raws_meta[\"raws_weather_vars\"]\n",
    "df_temp = synoptic.TimeSeries(\n",
    "        stid=\"HSYN1\",\n",
    "        start=start-relativedelta(hours=1),\n",
    "        end=end+relativedelta(hours=1),\n",
    "        vars=weather_vars,\n",
    "        units = \"metric\"\n",
    "    ).df()\n",
    "\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64d531-316c-46a8-8106-b6ffddadfbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat, units = rr.format_raws(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ed1f7-4592-46df-b2b8-9c1988444d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a6dc3-cdf3-42c6-ae3e-b58574f5d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a326d07-edf5-4285-a7c0-e81caf1774ed",
   "metadata": {},
   "source": [
    "We then loop over the station IDs found in the previous step and retrieve all available data and then rename and pivot from long to wide. The loop generates a dictionary for each RAWS station with keys for weather data and other metadata.\n",
    "\n",
    "*NOTE*: this process is not parallelized, as the same IP address is used for each request and parallelization may result in issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb05f7b-24d7-4811-9b12-d16764d56561",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Attempting retrieval of RAWS from {start} to {end} within {bbox}\")\n",
    "print(\"~\"*75)\n",
    "\n",
    "raws_dict = {}\n",
    "\n",
    "for st in sts[\"stid\"]:\n",
    "    print(\"~\"*50)\n",
    "    print(f\"Attempting retrival of station {st}\")\n",
    "    try:\n",
    "        df = synoptic.TimeSeries(\n",
    "            stid=st,\n",
    "            start=start-relativedelta(hours=1),\n",
    "            end=end+relativedelta(hours=1),\n",
    "            vars=weather_vars,\n",
    "            units = \"metric\"\n",
    "        ).df()\n",
    "    \n",
    "        dat, units = rr.format_raws(df)\n",
    "        loc = rr.get_static(sts, st)\n",
    "        raws_dict[st] = {\n",
    "            'RAWS': dat,\n",
    "            'units': units,\n",
    "            'loc': loc,\n",
    "            'misc': \"Data retrieved using `synoptic.TimeSeries` and formatted with custom functions within `ml_fmda` project.\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"An error occured: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc95c3b-bf47-4d85-a3df-919d23a833d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea92b6-4d7a-453d-adf0-46495e761f57",
   "metadata": {},
   "source": [
    "### Fix Time, Interpolate, and Calculate Rain\n",
    "\n",
    "Synoptic may return RAWS data that has missing hours or is returned not exactly on the hour. The missing hours are simply absent in the return data from Synoptic, not marked by NaN. We fix that by filling in NaN for missing hours and interpolating to the exact hour. The resulting data should have regular hourly observations for every RAWS station. If Synoptic returns only a small number of observations, the interpolation process may create long stretches of perfectly linear data from the interpolation. These stretches of suspect data are flagged and filtered in a later stage of the data processing in this project, since the hyperparameters controlling that filtering may be changed but the underlying retrieval and interpolation would be unchanged.\n",
    "\n",
    "Also, this is a good place in the code to rename variables. Various data sources have different variable names, so we standardize with naming conventions from the metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0249a5e-d4fc-4e9a-9830-2e7c5fe282f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = time_range(start, end, freq=\"1h\")\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c45935-c94c-4ceb-9232-7cb858e2e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raws_dict[\"BRLW4\"][\"RAWS\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea61230-cc2b-4f96-b104-23e4bbc87ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = rr.time_intp_df(raws_dict[\"BRLW4\"][\"RAWS\"], times)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f135c-d57f-4e5c-8086-4120e71eb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106051c-b6ae-45a2-aafe-aaf8445c6de7",
   "metadata": {},
   "source": [
    "We now loop over all stations and run temporal interpolation. We also convert to pandas for easier pickle write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd8c30-2cf0-4d45-9fac-3d0c09d1afb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Interpolating dataframe in time from {times.min()} to {times.max()}\")\n",
    "rename=True\n",
    "if rename:\n",
    "    print(f\"Renaming RAWS columns based on raws_metadata file\")\n",
    "for st in raws_dict:\n",
    "    print(\"~\"*75)\n",
    "    print(st)\n",
    "    nsteps = raws_dict[st][\"RAWS\"].shape[0]\n",
    "    raws_dict[st][\"RAWS\"] = rr.time_intp_df(raws_dict[st][\"RAWS\"], times)\n",
    "    raws_dict[st][\"RAWS\"] = pd.DataFrame(raws_dict[st][\"RAWS\"], columns = raws_dict[st][\"RAWS\"].columns)\n",
    "    raws_dict[st][\"times\"] = times\n",
    "    if raws_dict[st][\"RAWS\"].shape[0] != nsteps:\n",
    "        raws_dict[st][\"misc\"] += \" Interpolated data with numpy linear interpolation.\"\n",
    "        print(f\"    Original Dataframe time steps: {nsteps}\")\n",
    "        print(f\"    Interpolated DataFrame time steps: {raws_dict[st][\"RAWS\"].shape[0]}\")\n",
    "        print(f\"        interpolated {raws_dict[st][\"RAWS\"].shape[0] - nsteps} time steps\")\n",
    "    if rename:\n",
    "        raws_dict[st][\"units\"] = rename_dict(raws_dict[st][\"units\"], raws_meta[\"rename_synoptic\"])\n",
    "        raws_dict[st][\"RAWS\"] = raws_dict[st][\"RAWS\"].rename(columns = raws_meta[\"rename_synoptic\"])\n",
    "        raws_dict[st][\"loc\"] = rename_dict(raws_dict[st][\"loc\"], raws_meta[\"rename_synoptic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2457bf-0688-4147-b50d-c1d6c7b83181",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict[st].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7c95f-e7fd-43af-9c64-80625a318310",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict[st][\"units\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b8dc0-d6ff-407b-b43e-bca4593cc891",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict[st][\"loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac818850-b609-45dd-a77d-f5f356cc3ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict[st][\"misc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac512fe9-6309-45e1-bd85-86225123a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict[st][\"RAWS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c62973-a13d-465b-a85d-6b8deb79c89d",
   "metadata": {},
   "source": [
    "### Using Module Wrapper\n",
    "\n",
    "The module function `build_raws_dict_api` combines the previous steps. The resulting dictionary should be the same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a170120-c611-440b-a1ac-4b9b26628c3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raws_dict2 = rr.build_raws_dict_api(start, end, bbox, save_path = \"../data/raws_test1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8d82a-8655-49b9-8deb-9d50006cac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dicts\n",
    "np.all(raws_dict.keys() == raws_dict2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c716a4-4345-46df-a091-9b0894e03b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(raws_dict[\"WPKS2\"][\"RAWS\"] == raws_dict2[\"WPKS2\"][\"RAWS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd2571c-5dd0-46a3-870c-e22c349819d0",
   "metadata": {},
   "source": [
    "## RAWS Stash\n",
    "\n",
    "This is intended to be used for older data where the free Synoptic token won't return data. However, the stash needs to be unzipped and may not contain the latest data. Additionally, the stash only includes 10-hr dead FMC observations. It is a work in progress to save all other sensor variables in the stash. As of Jan 2025 this process will only return dead FMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dbcf75-9317-4a59-b902-d1ae3909d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = str2time('2023-01-01T00:00:00Z')\n",
    "end = str2time('2023-01-01T05:00:00Z')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0538616e-e9ad-4306-9176-f72db3612b82",
   "metadata": {},
   "source": [
    "### Get stash file paths\n",
    "\n",
    "Given a date range, it returns a list of file paths to read from the stash. Like before, we subtract an hour from the start and add an hour to the end to give the interpolation procedure endpoints outside the target time range. The file directories are arranged by year and Julian day (0-366). Then the individual files are for a single day and all RAWS available in CONUS at that time, saved as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524b2178-2eef-478e-a3aa-ba8da936c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = time_range(start-relativedelta(hours=1), end+relativedelta(hours=1))\n",
    "\n",
    "rr.get_file_paths(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f898790b-51f6-4460-9a31-1ac373da79ee",
   "metadata": {},
   "source": [
    "### Build Dictionary\n",
    "\n",
    "The process calls the `get_stations` function shown above (the one time where the API is used here), then loops through the files listed above and extracts data for the needed stations into a nested dictionary format that matches the format above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada2abee-9da4-4d16-915e-94f865978e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ingest.RAWS\n",
    "importlib.reload(ingest.RAWS)\n",
    "import ingest.RAWS as rr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7cf5d0-2409-47be-93c0-8159cb99adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ca751-0e81-4927-850e-9446dbffeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad1aa4-837b-43bf-b551-c37a74b58e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raws_dict3 = rr.build_raws_dict_stash(start, end, bbox, save_path = \"../data/raws_test2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517a15f-4b01-46df-9d38-73f7983c5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict3.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50bf28-066c-465a-b1e0-5d0b3815ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict3[\"BRLW4\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8318d-65ba-4b4f-b529-646cee4f981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict3[\"BRLW4\"][\"units\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63229eca-1fee-4b9c-bd96-c7a2fe857319",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict3[\"BRLW4\"][\"loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44376f64-c333-45e1-82c4-8abf755c5cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict3[\"BRLW4\"][\"RAWS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c29242-98c5-44d5-908c-a01d6346a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict3[\"BRLW4\"][\"misc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c5406-abc1-4fc0-86cc-dc9da8da157a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
