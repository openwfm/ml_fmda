{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f4cc18-d23f-48ea-84dc-b0328a65e8e5",
   "metadata": {},
   "source": [
    "# Build Climatology Tutorial\n",
    "\n",
    "As a baseline method, we build a climatology for FMC based on the historical average at a particular hour of the day. This method is inspired by Schreck et all 2023. This notebook utilizes retrieval and filtering of 10-h dead FMC data from a RAWS stash maintained by Angel Farguell, demonstrated in notebook `ingest_fm10_stash_tutorial`.\n",
    "\n",
    "Main processes:\n",
    "- `build_climatology`: this function retrieves RAWS data from a stash given input time period and spatial domain. The data is saved to a local directory for potential reuse, such as repeated applications of calculating forecast error for cross validation. Parameters for this process are stored in `etc/params_models.yaml`\n",
    "- `get_climatology_forecasts`: this function returns FMC forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed774d-7bef-48e7-b9a6-8f4ba4e17d81",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b32fd-9d6b-4582-b724-4d2a094a6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import json\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('../src')\n",
    "from utils import Dict, read_yml, read_pkl, str2time, print_dict_summary, time_range\n",
    "# import ingest.retrieve_raws_api as rr\n",
    "# import ingest.retrieve_raws_stash as rrs\n",
    "import models.moisture_models as mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd65b844-5584-4011-b6db-a2a3f0c1fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../etc/training_data_config.json\", \"r\") as json_file:\n",
    "    config = json.load(json_file)   \n",
    "    config = Dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b91e911-b0dd-4937-b566-c361a2004f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict_summary(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736f719a-82f2-486a-9178-26bcf23b1c30",
   "metadata": {},
   "source": [
    "## Climatology\n",
    "\n",
    "Method description...\n",
    "\n",
    "6 years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb342b0a-9881-4c51-8209-9c50e30dfc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = str2time(config.start_time)\n",
    "# end = start + relativedelta(hours=48)\n",
    "end = start + relativedelta(hours = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301af779-e8cf-4118-aa43-4ecf26d27402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import models.moisture_models\n",
    "importlib.reload(models.moisture_models)\n",
    "import models.moisture_models as mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f97cc-3417-4c3e-9394-e8ad9d44597e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clim_dict = mm.build_climatology(\n",
    "    start,\n",
    "    end,\n",
    "    config.bbox\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135ba69-0224-498c-9ab9-aed8e129043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_df = mm.get_climatology_forecasts(clim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfbc7d6-8417-4eba-8347-afd23beeaa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f5bf4-6c1a-4369-ab8a-3930d9c0d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a few timeseries\n",
    "\n",
    "df_subset = clim_df.head(5).T  \n",
    "plt.figure(figsize=(10, 6))\n",
    "for idx in df_subset.columns:\n",
    "    plt.plot(df_subset.index, df_subset[idx], label=str(idx))\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Forecasted FMC (%)')\n",
    "plt.title('Forecasted FMC over Time')\n",
    "plt.legend(title='STID')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6eef6d-d755-4df6-a4fb-d7f75b720e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf0068a-0bb6-46d9-b429-de94557e66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from ingest.RAWS import get_file_paths, get_stations\n",
    "from utils import Dict, read_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c530c-ff7d-46e4-bb60-b1b2d39588ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_params = mm.clim_params\n",
    "clim_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966db12b-5f71-4658-88f3-8de7c91904cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_climatology(start, end, bbox, nyears=clim_params.nyears, ndays=clim_params.ndays, min_years = clim_params.min_years):\n",
    "    \n",
    "    def count_years(values_df, times_df):\n",
    "        \"\"\"\n",
    "        Based on years in times_df, count number of non-nan values per year in values_df. \n",
    "        Result should be a count of the number of years of data with non-nan\n",
    "        \"\"\"\n",
    "        counts = {\n",
    "            col: times_df[values_df[col].notna()][col].nunique()\n",
    "            for col in values_df.columns\n",
    "        }\n",
    "        counts = pd.Series(counts)\n",
    "        return counts        \n",
    "\n",
    "    # Retrieve data\n",
    "    ## Note, many station IDs will be empty, the list of stids was for the entire bbox region in history\n",
    "    print(f\"Retrieving climatology data from {start} to {end}\")\n",
    "    print(\"Params for Climatology:\")\n",
    "    print(f\"    Number of years to look back: {nyears}\")\n",
    "    print(f\"    Number of days to bracked target hour: {ndays}\")\n",
    "    print(f\"    Required number of years of data: {min_years}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58743f7-c9ea-42ee-8e43-02c49a3569fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_climatology(start, end, config.bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e29cf-6fb8-4301-9e35-4629fd8efccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_df = get_stations(config.bbox)\n",
    "sts = list(sts[\"stid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb6183-35fd-4973-a2e2-6b1f0ccb42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast Times\n",
    "ftimes = time_range(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac0fff-431e-4e7d-995b-d979b12835f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earliest time associated with forecast time,.\n",
    "# In this project, 1 year previous to earliest forecast time\n",
    "# and 15 days before that based on 30 day window\n",
    "t0 = ftimes.min() - relativedelta(years=clim_params.nyears) - relativedelta(days = clim_params.ndays)\n",
    "t1 = ftimes.max()\n",
    "\n",
    "all_times = time_range(t0, t1)\n",
    "print(f\"Total hours to retrieve for climatology: {len(all_times)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c7880-d7e7-4187-b0d3-0987d38180c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_files = get_file_paths(all_times)\n",
    "raws_files = [f for f in raws_files if os.path.exists(f)]\n",
    "print(f\"Existing RAWS Files: {len(raws_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb1272-609a-44ab-adcf-af037b66bd1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b482b5db-2445-4758-bf0d-12b30faf3dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = pd.read_pickle(raws_files[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153caba0-3d28-412f-9117-80a9137c34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_pickle(file_path, sts):\n",
    "    \"\"\"Load a pickle file using pd.read_pickle and filter by 'stid' column.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)  # Use pd.read_pickle instead of pickle.load\n",
    "        df.columns = df.columns.str.lower()\n",
    "        if isinstance(df, pd.DataFrame) and \"stid\" in df.columns:\n",
    "            return df[df[\"stid\"].isin(sts)]  # Filter rows\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")  # Handle errors gracefully\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed414a8-8a1f-44bd-ada1-0ca911d522e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_filter_pickle(raws_files[80000], sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2aedd-6f4e-4984-9dda-9ea19bb3e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parallel_load_pickles(file_list, sts, num_workers=8):\n",
    "#     \"\"\"Parallel loading and filtering of pickle files using pd.read_pickle.\"\"\"\n",
    "#     with Pool(num_workers) as pool:\n",
    "#         results = pool.starmap(load_and_filter_pickle, [(f, sts) for f in file_list])\n",
    "    \n",
    "#     # Concatenate all filtered DataFrames, ignoring None values\n",
    "#     return pd.concat([df for df in results if df is not None], ignore_index=True)\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "def parallel_load_pickles(file_list, sts, num_workers=8):\n",
    "    \"\"\"Parallel loading using joblib instead of multiprocessing.\"\"\"\n",
    "    results = Parallel(n_jobs=num_workers)(delayed(load_and_filter_pickle)(f, sts) for f in file_list)\n",
    "    return pd.concat([df for df in results if df is not None], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f98038-8075-4a9e-a28a-771ce63f089c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clim_data = parallel_load_pickles(raws_files, sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b8f6b-ac67-450f-9d10-0d11f29c012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc42bf2-3c9f-4c74-a783-ed2f7171e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f09db-fae5-4150-948e-be00d39c3a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_data_dir = \"../data/climatology\"\n",
    "filename = f\"test_climatology_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32235f8-2b0c-4bb7-9ba1-de4cd8e05aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(clim_data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd4cf4-7d1d-4af1-a183-179e0265479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(osp.join(clim_data_dir, f\"{filename}.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(clim_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0ecbf-29ce-4a5d-abe3-c7246b1e7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_metadata = {\n",
    "    'start': start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'end': end.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'bbox': config.bbox,\n",
    "    'nyears': clim_params.nyears,\n",
    "    'min_years': clim_params.min_years,\n",
    "    'ndays (+/-)': clim_params.ndays\n",
    "}\n",
    "clim_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df78df-2b98-4239-808e-4d24afdcaf7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f973b-e13e-4154-9bed-51367875bb92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e9cf2-bce5-43ef-9608-c85312d405f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(osp.join(clim_data_dir, f\"{filename}_metadata.txt\"), \"w\") as f:\n",
    "    for key, value in clim_metadata.items():\n",
    "        f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c58b2-8cb9-4d6a-86b7-84b4927b1598",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03253e2d-6cbd-42b9-baab-9c07f170bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06dd4a-8ca2-436a-a885-de74585d6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084dbc0e-0258-4489-bb24-677ceccedd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1d3f8-58c5-47e4-b61c-ef2c6be9d56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f25a84-f3d1-403a-afbb-95f19c26e092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ti = mm.time_to_climtimes(ftimes[0], nyears = clim_params.nyears, ndays=clim_params.ndays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4044ce6-cbba-4b8b-8458-05de1213b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707748a-b725-4c8d-886b-6488ca76b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03edd68e-deb6-4d3a-81ab-778d92c90599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81eaf06-c67e-4983-aef3-0e341c0869be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_clim_data(clim_data, clim_times):\n",
    "    \"\"\"\n",
    "    Filters clim_data to include only rows where the 'datetime' column matches \n",
    "    any datetime in clim_times based on year, month, day, and hour.\n",
    "    \n",
    "    Parameters:\n",
    "    - clim_data (pd.DataFrame): DataFrame containing 'datetime' column (numpy datetime64).\n",
    "    - clim_times (np.ndarray): Array of datetime objects to match.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # Convert clim_times to a DataFrame for efficient merging\n",
    "    clim_times_df = pd.DataFrame({\n",
    "        \"year\": [t.year for t in clim_times],\n",
    "        \"month\": [t.month for t in clim_times],\n",
    "        \"day\": [t.day for t in clim_times],\n",
    "        \"hour\": [t.hour for t in clim_times]\n",
    "    }).drop_duplicates()  # Remove duplicates to speed up filtering\n",
    "\n",
    "    # Extract the relevant time components from clim_data\n",
    "    clim_data_filtered = clim_data.assign(\n",
    "        year=clim_data[\"datetime\"].dt.year,\n",
    "        month=clim_data[\"datetime\"].dt.month,\n",
    "        day=clim_data[\"datetime\"].dt.day,\n",
    "        hour=clim_data[\"datetime\"].dt.hour\n",
    "    ).merge(clim_times_df, on=[\"year\", \"month\", \"day\", \"hour\"], how=\"inner\")\n",
    "\n",
    "    return clim_data_filtered.drop(columns=[\"year\", \"month\", \"day\", \"hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c4b53-02ac-40dd-bfd1-4adfeba4759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = filter_clim_data(clim_data, ti)\n",
    "dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e701022-729d-48d2-949d-11c071a9bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def average_fm10_by_stid(filtered_df, min_years):\n",
    "    \"\"\"\n",
    "    Computes the average fm10 grouped by 'stid', but returns NaN if the number \n",
    "    of unique years in 'datetime' is less than nyears.\n",
    "\n",
    "    Parameters:\n",
    "    - filtered_df (pd.DataFrame): DataFrame containing 'stid', 'datetime', and 'fm10'.\n",
    "    - nyears (int): Minimum number of unique years required per 'stid'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: Averaged fm10 per 'stid' (NaN if unique years < nyears).\n",
    "    \"\"\"\n",
    "    # Extract unique years for each STID\n",
    "    year_counts = filtered_df.groupby(\"stid\")[\"datetime\"].apply(lambda x: x.dt.year.nunique())\n",
    "\n",
    "    # Compute fm10 average per STID\n",
    "    fm10_avg = filtered_df.groupby(\"stid\")[\"fm10\"].mean()\n",
    "\n",
    "    # Set to NaN where unique years < nyears\n",
    "    fm10_avg[year_counts < min_years] = np.nan\n",
    "\n",
    "    return fm10_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49be6bc-5b6f-4622-b181-11d9d698cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = average_fm10_by_stid(dfi, min_years = clim_params.min_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329fc2c-a2b8-4a4f-95a2-1db2b3db217a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7c1f7-4cae-4f36-98d6-159c1ccd68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ftimes(ftimes, clim_data, clim_params):\n",
    "    \"\"\"\n",
    "    Runs `time_to_climtimes` on each time in `ftimes`, filters `clim_data`,\n",
    "    computes the average `fm10` per `stid`, and combines results.\n",
    "\n",
    "    Parameters:\n",
    "    - ftimes (np.ndarray): Array of datetime objects to process.\n",
    "    - clim_data (pd.DataFrame): DataFrame containing 'stid', 'datetime', and 'fm10'.\n",
    "    - clim_params: Object containing `nyears` and `ndays` parameters.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Combined results with average fm10 per stid for each ftime.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for ftime in ftimes:\n",
    "        # Generate climtimes for the given ftime\n",
    "        clim_times = mm.time_to_climtimes(ftime, nyears=clim_params.nyears, ndays=clim_params.ndays)\n",
    "        \n",
    "        # Filter clim_data based on clim_times\n",
    "        filtered_data = filter_clim_data(clim_data, clim_times)\n",
    "\n",
    "        # Compute the average fm10 per stid\n",
    "        fm10_avg = average_fm10_by_stid(filtered_data, min_years=clim_params.min_years)\n",
    "\n",
    "        # Store results with corresponding ftime\n",
    "        df_result = fm10_avg.reset_index()\n",
    "        df_result[\"forecast_time\"] = ftime  # Add ftime column\n",
    "        results.append(df_result)\n",
    "\n",
    "    # Combine all results into a single DataFrame\n",
    "    combined_df = pd.concat(results, ignore_index=True)\n",
    "    pivot_df = combined_df.pivot(index=\"stid\", columns=\"forecast_time\", values=\"fm10\")    \n",
    "\n",
    "    return pivot_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c680319-367d-46e5-9940-e64c3be15213",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = process_ftimes(ftimes, clim_data, clim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6575972b-6ac7-4fe1-b239-935cda2e677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_stids = x.index[x.isna().all(axis=1)].tolist()\n",
    "x = x.dropna(how=\"all\")\n",
    "print(dropped_stids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1532ba5f-7e20-483d-a1f9-45095592f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc6cb28-c252-4f15-bf68-23a304f11f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
