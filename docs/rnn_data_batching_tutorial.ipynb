{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f4cc18-d23f-48ea-84dc-b0328a65e8e5",
   "metadata": {},
   "source": [
    "# Build Machine Learning Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79db2ac-aa72-4e5f-85e0-e21def49917d",
   "metadata": {},
   "source": [
    "The data retrieval process loops through a range of dates, retrieves and joins RAWS, HRRR, and other data sources and saves to a local directory.\n",
    "\n",
    "This notebook describes the process of reading that data, performing the final set of quality control filters, and formatting into data that can be fed into the various models used in this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed774d-7bef-48e7-b9a6-8f4ba4e17d81",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b32fd-9d6b-4582-b724-4d2a094a6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from datetime import datetime, timezone\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import synoptic\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "sys.path.append('../src')\n",
    "import reproducibility\n",
    "from utils import Dict, read_yml, read_pkl, str2time, print_dict_summary, time_range, rename_dict\n",
    "import models.moisture_models as mm\n",
    "import models.moisture_rnn as mrnn\n",
    "from models.moisture_models import XGB, LM\n",
    "import ingest.RAWS as rr\n",
    "import ingest.HRRR as ih\n",
    "import data_funcs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba6476-fbc3-4dff-93e3-ab0550a3ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"2023-01-01T00:00:00Z\"\n",
    "end = \"2023-01-06T23:00:00Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bffca74-8c6d-45b0-9726-cd2a0687a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_data = Dict(read_yml(\"../etc/params_data.yaml\"))\n",
    "print_dict_summary(params_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05eb21-3b84-4de1-96a9-40ee2826b851",
   "metadata": {},
   "source": [
    "## Read Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c52141-2a6d-454c-b4ea-f6a46e20b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = read_pkl(\"../data/test_data/test_ml_dat.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176544c-e363-44c9-baf1-732e09067a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339e91d-6c3e-4134-ba22-651c4a3ee790",
   "metadata": {},
   "source": [
    "## Setup CV\n",
    "\n",
    "Steps:\n",
    "* Determine time ranges for train/val/test\n",
    "* Get stations with data availability in those periods\n",
    "* Sample stations for train/val/test\n",
    "\n",
    "\n",
    "Different stations will have different gaps of data availability for the train/val/test time periods. When selecting stations for inclusion in those periods, we use the following methodology:\n",
    "* Let $N$ be the total number of stations that returned data over the combined train/val/test times\n",
    "* Let $N_t$ be the number of stations included in each of the validation and test sets, and are chosen to be the nearest integer to 10\\% of $N$\n",
    "* Starting with the test time period, we select $N_t$ of the number of stations with data availability in the period. In other words, there may be less than $N$ stations with data availability in the test period, but we select $N_t$ if possible\n",
    "* Then, we select $N_t$ stations for inclusion in the validation set, excluding any of the $N_t$ stations included in the test set\n",
    "* Finally, we use any remaining stations for the training set that weren't included in either of the validation or test sets. So there is a maximum of $N-2\\cdot N_t$ stations included in the training set \n",
    "\n",
    "This methodology makes it so the number of stations included in the training set varies and is sometimes less than $N-2\\cdot N_t$. We fix the number of stations in the test and validation sets and allow the number of stations in the training set to vary. This is because we don't want accuracy metrics to be calculated consistently for those periods. If there are fewer stations with data availability for a certain period, we want that be to reflected in a smaller training set and presumably less accurate metrics on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8357373-00b7-44cb-a91e-b79fe8319e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_times, val_times, test_times = data_funcs.cv_time_setup(\"2023-01-29T00:00:00Z\", \n",
    "                                                train_hours=24*28, forecast_hours=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff1767-11c3-46da-9b44-7032662660ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_sts, val_sts, te_sts = data_funcs.cv_space_setup(dat, \n",
    "                                                    val_times=val_times, \n",
    "                                                    test_times=test_times, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fd15e-5087-47d7-80f2-681bd0820eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85a83f-091f-41b3-9536-fb5366eb0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(te_sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68109f3c-2248-4360-ad46-e11a98f4389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_funcs.get_sts_and_times(dat, tr_sts, train_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d75b9-d169-471e-b616-f4138634f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = data_funcs.get_sts_and_times(dat, val_sts, val_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be4977-e533-4bf2-b3d2-f60f02d53f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_funcs.get_sts_and_times(dat, te_sts, test_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d49697-2a7f-412f-aef8-c12f739e027a",
   "metadata": {},
   "source": [
    "## Batching Data for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135c547-11a9-491c-a3f8-78e54b6c149c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for st in train:\n",
    "    print(train[st][\"data\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589b128-d71f-4584-a073-1dd2045be57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all samples with staircase\n",
    "\n",
    "X, y, t = mrnn.staircase_dict(train, sequence_length=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f5319-2e26-4615-b936-6bc2a10af022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for Xi in X:\n",
    "    print(Xi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93904fd3-bab6-4855-acf3-be7790e749cd",
   "metadata": {},
   "source": [
    "### Stateful batching\n",
    "\n",
    "The algorithm is too complex for variable number of samples at different times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8e018-9abd-4eb8-80bf-90e8227d1912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stateful_batches(X_list, y_list, batch_size = 32, timesteps=12, \n",
    "                           return_sequences=False, start_times=\"zeros\", verbose=True):\n",
    "    \"\"\"\n",
    "    Construct data for RNN training (and validation data) with format (batch_size, timesteps, features) \n",
    "    Intended to be run on train set and validation set (if using)\n",
    "\n",
    "    Given list of staircase structured data, i.e. output of staircase_dict, create batches by getting samples from\n",
    "    each list element, so samples within a batch are from different physical locations.\n",
    "\n",
    "    If start_times is zeros, in the first batch, and any new batch with all new locations, select the 0th (aka first in python)\n",
    "    sample to build for the batch.\n",
    "\n",
    "    Args:\n",
    "        - X_list: (list) list of numpy ndarrays of predictors\n",
    "        - y_list: (list) list of numpy ndarrays of response data\n",
    "        - batch_size: (int) number of samples of length timesteps to include in a single iteration of weight updates\n",
    "        - timesteps: (int) number of discrete time steps that defines a single sample\n",
    "        - return_sequences: (bool) Whether to include all response y values for timesteps, or just last step\n",
    "        - start_times: if \"zeros\" all samples start at time 0. (Only one for now)\n",
    "    Returns:\n",
    "        XX, yy: tuple of structured predictors and outcomes variables. \n",
    "            XX shape will be (num_samples, timesteps, features), where num_samples determined by batch size and input X length\n",
    "            yy shape will be (num_samples, 1) OR (num_samples, timesteps) if return sequences\n",
    "    \"\"\"\n",
    "\n",
    "    # Run some checks\n",
    "    if len(X_list) != len(y_list):\n",
    "        raise ValueError(f\"Mismatch data. {len(X_list)=}, {len(y_list)=}. Check they were created together\")\n",
    "    if len(X_list) < batch_size:\n",
    "        raise ValueError(f\"Batch size greter than number of locations. Method not implemented for this, try a smaller batch size. {len(X_list)=}, {batch_size=}.\")\n",
    "\n",
    "    # Set up return objects    \n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    loc_batches = []\n",
    "    t_batches = []\n",
    "    \n",
    "    # Set up indices for first batch\n",
    "    loc_index = np.arange(batch_size)\n",
    "    loc_counter = loc_index.max() # used to iterate to new locations\n",
    "    loc_resets = []\n",
    "    X_set = set(np.arange(len(X_list)))\n",
    "    # t_index0 = np.arange(batch_size) # used to reset times on new location\n",
    "    # t_index = np.arange(batch_size)\n",
    "    t_index0 = np.zeros(batch_size)\n",
    "    t_index = np.zeros(batch_size)\n",
    "    \n",
    "    b = 0 # batch index     \n",
    "    run = True\n",
    "    while run:\n",
    "        print(\"~\"*75)\n",
    "        print(f\"Batch {b}:\")\n",
    "\n",
    "        print(f\"Location Indices: {loc_index}\")\n",
    "        print(f\"Time Indices: {t_index}\")\n",
    "        \n",
    "        # Get data\n",
    "        X_batch = np.array([X_list[loc][int(t)] for loc, t in zip(loc_index, t_index)])\n",
    "        y_batch = np.array([y_list[loc][int(t)] for loc, t in zip(loc_index, t_index)])\n",
    "        if not return_sequences:\n",
    "            y_batch = y_batch[:, -1, :] # Get last time step of sequence\n",
    "\n",
    "        # Save batch info by appending\n",
    "        X_batches.append(X_batch.copy())\n",
    "        y_batches.append(y_batch.copy())\n",
    "        t_batches.append(t_index.copy())\n",
    "        loc_batches.append(loc_index.copy())\n",
    "        \n",
    "        # Update indices for next iteration\n",
    "        t_index += timesteps # iterate time index by timesteps param\n",
    "\n",
    "        # Check times and locations, adjust if needed\n",
    "        for i in range(0, len(loc_index)):\n",
    "            loci = loc_index[i]\n",
    "            ti = t_index[i]\n",
    "            Xi = X_list[loci]\n",
    "            if Xi.shape[0] <= ti:\n",
    "                # Condition triggered that requested time index is \n",
    "                # greater than samples available for given location\n",
    "                # So iterate location index and reset time to t_index0\n",
    "                t_index[i] = t_index0[i]\n",
    "                loc_counter += 1\n",
    "                new_loc_i = loc_counter % len(X_list)\n",
    "                loc_resets.append(loc_index[i].copy()) # Keep track of which locations get reset\n",
    "\n",
    "                if not set(loc_resets) - X_set:                \n",
    "                    # Condition triggered when maximum loc index has been reset to 0\n",
    "                    # Indicates we have cycles through all locations, STOP\n",
    "                    print(f\"Stopping at batch {b}\")\n",
    "                    run = False\n",
    "                    break\n",
    "                loc_index[i] = new_loc_i\n",
    "                print(f\"Changing location {i} index to: {new_loc_i}\")\n",
    "                print(f\"    With Time index to: {t_index0[i]}\")\n",
    "        \n",
    "        b += 1 # iterate batch\n",
    "\n",
    "\n",
    "    # return np.array(X_batches), np.array(y_batches), t_batches, loc_batches\n",
    "    return np.concatenate(X_batches, axis=0), np.concatenate(y_batches, axis=0), t_batches, loc_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c14999-9b74-4faf-8eed-0a5318826b05",
   "metadata": {},
   "source": [
    "## RNN Data\n",
    "\n",
    "For training RNNs (simple, LSTM, GRU included), the data must be structured as `(batch_size, timesteps, features)`. So a single \"sample\" in this context is a timeseries of length `timesteps` and dimensionality `features`. RNNs can be trained with different size timesteps and batch sizes, which is often useful in the context of natural language processing. However, if running an RNN in \"stateful\" model, which maintains the dependence between different samples from the same location, the data must have consistent number of timesteps and batch size across all inputs. Further, when using static features like lon/lat or elevation, it is desirable to have samples from different locations within the same batch. Otherwise, if a batch is constructed with samples all from the same location, the static features will have zero variance for a given batch and the model cannot learn any relationship between the static features and the outcome variable for that batch.\n",
    "\n",
    "Data is stored in a custom class `RNNData` defined in `models/moisture_rnn.py`. A custom class is used to organize scaling as well as batch construction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3df6f7-9f67-46fc-ab0b-a6df66fb4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = mrnn.RNNData(train, val, test,\n",
    "                  method=\"random\", random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812c8a6-e2b2-48e8-a3e3-b2dbd639eae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978b5f8-76a6-4cde-884d-3bd772ae90b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f3d08-6dc7-4708-bff9-aa4f66864793",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dat.X_train, axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed2858d-060d-40c7-b269-45448308c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dat.X_val, axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d4126-300b-4020-ae82-6c5c096d404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dat.X_test, axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3ba43-4e51-4f4f-bb85-49ac8a4d4b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e04d56-e9b0-46ea-aabf-57c7c7ba9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.scale_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e66d8f-c7a9-4515-9080-6c919f0ae7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dat.X_train, axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a887fd-8054-403b-a08b-dcc3298e8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dat.X_val, axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c57a9-ef37-4c97-8d91-a7eebb7b0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dat.X_test, axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf788108-3ab9-4e06-aea4-2d1df3282173",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a64420-e01d-437c-990e-ef5f70c55a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94cb59-b6c1-4786-befb-486dee7eee08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875e47b-1700-440e-a0c0-2176a895b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = dat.inverse_scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dacc6b-7c57-4d03-8576-09eb71824262",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(a, axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6623ad15-f825-4ecb-a4b3-be3fb0319a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(b, axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4301f14-1a5c-40b9-9a96-165e396c2b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(c, axis=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7097c99-44c8-439b-a5bc-890f33720f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027a28ab-5faa-4c1a-aac4-f992eff02ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e7add-dfe3-4c1f-872a-1825037dac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db24fa0-4753-4472-b6d5-be520c94bc78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cec032-55f7-4f47-a10f-7d5a173d5709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca5f0c6-6db4-44e6-a179-5ebcb5799506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a72acd8-a33e-4a6d-9972-4f96ffabbbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591edd8-4b81-4c63-9e6b-aa4708ab0012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
