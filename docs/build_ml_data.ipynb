{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f4cc18-d23f-48ea-84dc-b0328a65e8e5",
   "metadata": {},
   "source": [
    "# Build Machine Learning Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79db2ac-aa72-4e5f-85e0-e21def49917d",
   "metadata": {},
   "source": [
    "The data retrieval process loops through a range of dates, retrieves and joins RAWS, HRRR, and other data sources and saves to a local directory.\n",
    "\n",
    "This notebook describes the process of reading that data, performing the final set of quality control filters, and formatting into data that can be fed into the various models used in this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed774d-7bef-48e7-b9a6-8f4ba4e17d81",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b32fd-9d6b-4582-b724-4d2a094a6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from datetime import datetime, timezone\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import synoptic\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "sys.path.append('../src')\n",
    "from utils import Dict, read_yml, read_pkl, str2time, print_dict_summary, time_range, rename_dict\n",
    "import models.moisture_models as mm\n",
    "import ingest.RAWS as rr\n",
    "import ingest.HRRR as ih\n",
    "import data_funcs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba6476-fbc3-4dff-93e3-ab0550a3ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"2023-01-01T00:00:00Z\"\n",
    "end = \"2023-01-06T23:00:00Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bffca74-8c6d-45b0-9726-cd2a0687a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_data = Dict(read_yml(\"../etc/params_data.yaml\"))\n",
    "print_dict_summary(params_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05eb21-3b84-4de1-96a9-40ee2826b851",
   "metadata": {},
   "source": [
    "## Retrieve Data\n",
    "\n",
    "Nested dictionary with top level key corresponding to a RAWS and subkeys for RAWS, atmospheric data (HRRR), geographic info, etc\n",
    "\n",
    "This format is used because different FMC models used in this project require different data formatting. The ODE+KF physics-based model is run pointwise and does not incorporate info from other locations. The static ML models have the least restrictive input data structure, and all observations can be thrown into one set of tabular data. The RNN models require structuring input data with the format (batch_size, timesteps, features). Thus, it is simpler to keep all data separate at separate locations and recombine in various ways at the modeling step. Also, data filters for suspect RAWS sensors are applied in the next step. This is because the raw data retrieval should not depend on hyperparameter choices related to data filters, so it is easier to collect everything and apply filters later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0547a5-0e20-4deb-a42b-524d78315726",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"../data/rocky_fmda/202301/fmda_20230101.pkl\", \n",
    "         \"../data/rocky_fmda/202301/fmda_20230102.pkl\",\n",
    "         \"../data/rocky_fmda/202301/fmda_20230103.pkl\",\n",
    "         \"../data/rocky_fmda/202301/fmda_20230104.pkl\",\n",
    "         \"../data/rocky_fmda/202301/fmda_20230105.pkl\",\n",
    "         \"../data/rocky_fmda/202301/fmda_20230106.pkl\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62448d6-7294-4cbd-9570-978c04635e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data_funcs\n",
    "importlib.reload(data_funcs)\n",
    "from data_funcs import combine_fmda_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b41eb6-099b-453a-9afa-8547ddd5966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict = data_funcs.combine_fmda_files(paths, save_path=\"../data/test_data/test_fmda_combined.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11870378-f722-423e-9ab7-82ca9dfdad2e",
   "metadata": {},
   "source": [
    "## Build ML Dataset\n",
    "\n",
    "Filter data and merge RAWS and HRRR and other sources. The file `etc/params_data.yaml` has hyperparameters related to filtering data. The steps include:\n",
    "\n",
    "- Determine atmospheric data source. Intended to be \"HRRR\" for production, but \"RAWS\" used for research purposes.\n",
    "- Combine atmospheric data predictors with FMC\n",
    "- Break timeseries into 72 hour periods, adding a column \"st_period\" starting at 0 (see README for info on why 72)\n",
    "- Apply data filters to 72 hour periods to RAWS data and remove from samples. HRRR data should already be QC'ed, so filtering will not be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176544c-e363-44c9-baf1-732e09067a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b58a8-6f41-4ade-9a3e-b92d4f789b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ml_dict = data_funcs.build_ml_data(raws_dict, hours=params_data.hours, \n",
    "                                   max_linear_time = params_data.max_linear_time, \n",
    "                                   save_path = \"../data/test_data/test_ml_dat.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2134f539-6cb4-4015-bd12-e250111fd78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raws_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722c8ac-3e73-437c-a014-5995a0fa2cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ml_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437d6d3-6b96-48a0-ade3-6c77f4aa383f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f339e91d-6c3e-4134-ba22-651c4a3ee790",
   "metadata": {},
   "source": [
    "## Setup CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8357373-00b7-44cb-a91e-b79fe8319e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_times, val_times, test_times = data_funcs.cv_time_setup(\"2023-01-05T00:00:00Z\", \n",
    "                                                train_hours=48*2, forecast_hours=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff1767-11c3-46da-9b44-7032662660ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "stids = [*ml_dict.keys()]\n",
    "\n",
    "tr_sts, val_sts, te_sts = data_funcs.cv_space_setup(stids, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fd15e-5087-47d7-80f2-681bd0820eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85a83f-091f-41b3-9536-fb5366eb0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(te_sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68109f3c-2248-4360-ad46-e11a98f4389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_funcs.get_sts_and_times(ml_dict, tr_sts, train_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d75b9-d169-471e-b616-f4138634f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = data_funcs.get_sts_and_times(ml_dict, val_sts, val_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be4977-e533-4bf2-b3d2-f60f02d53f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_funcs.get_sts_and_times(ml_dict, te_sts, test_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568c10d7-fc06-4c89-aa22-8f6e4de7519d",
   "metadata": {},
   "source": [
    "## ODE+KF Data\n",
    "\n",
    "* Run on 72 hour stretches (24 spinup, 48 val)\n",
    "* Get test station list used by other models\n",
    "* For those test stations, use `get_sts_and_times` accounting for the spinup period\n",
    "    * So adjust test times by subtracting 24 hours to account for spinup\n",
    " \n",
    "Function `get_ode_data` wraps the `get_sts_and_times` function... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c58b72-feb2-4892-973b-7fb7eef71be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_data = data_funcs.get_ode_data(ml_dict, te_sts, test_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766c087-84ef-46b6-8dd8-95c5ff5e1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ode = mm.ODE_FMC()\n",
    "m, errs = ode.run_model(ode_data, hours=72, h2=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149cf12c-db9a-4f32-8070-cf4bd3c9cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE Over Test Period: {errs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba43ee3-1fda-48bd-9198-e2aa5cb607ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cabc3c8f-8789-4d9c-a53f-0eabc7b88fd2",
   "metadata": {},
   "source": [
    "## Static ML Data\n",
    "\n",
    "Throw all train/val/test data together without worrying about timesteps samples. In other words, data can all be jumbled up in any order as observations are considered independent in time.\n",
    "\n",
    "Data is stored as a custom class `StaticMLData` defined in `models/moisture_models.py`. A custom class is used to organize data scaling and inverse scaling. A scaler should be fit using only the training data, and then applied to the val and test data to avoid data leakage. This is done internally in the StaticMLData class. Additionally, the class has methods to print hashes for reproducibility checks, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595dfe4c-9ba3-4c3f-9643-225885a77b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Dictionary of scalers, used to avoid multiple object creation and to avoid multiple if statements\n",
    "scalers = {\n",
    "    'minmax': MinMaxScaler(),\n",
    "    'standard': StandardScaler() \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a6ae96-53f6-4102-b03f-25a3c4031e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from utils import hash_ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1de4a-9b52-4c2f-8ffc-132b3409b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLData(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for ML Data, providing support for scaling. \n",
    "    Scaling performed on training data and applied to val and test.\n",
    "    \"\"\"    \n",
    "    def __init__(self, train, val=None, test=None, scaler=\"standard\", features_list=None):\n",
    "        self._run_checks(train, val, test, scaler)\n",
    "\n",
    "        if scaler not in {\"standard\", \"minmax\"}:\n",
    "            raise ValueError(\"scaler must be 'standard' or 'minmax'\")\n",
    "        self.scaler = StandardScaler() if scaler == \"standard\" else MinMaxScaler()\n",
    "        self.features_list = features_list if features_list is not None else [\"Ed\", \"Ew\", \"rain\"]\n",
    "\n",
    "        # Setup data fiels, e.g. X_train and y_train\n",
    "        self._setup_data(train, val, test)\n",
    "        # Assuming that units are all the same as it was checked in a previous step\n",
    "        self.units = next(iter(train.values()))[\"units\"]\n",
    "    \n",
    "    def _run_checks(self, train, val, test, scaler):\n",
    "        \"\"\"Validates input types for train, val, test, and scaler.\"\"\"\n",
    "        if not isinstance(train, dict):\n",
    "            raise ValueError(\"train must be a dictionary\")\n",
    "        if val is not None and not isinstance(val, dict):\n",
    "            raise ValueError(\"val must be a dictionary or None\")\n",
    "        if test is not None and not isinstance(test, dict):\n",
    "            raise ValueError(\"test must be a dictionary or None\")\n",
    "        if scaler not in {\"standard\", \"minmax\"}:\n",
    "            raise ValueError(\"scaler must be 'standard' or 'minmax'\")\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _setup_data(self, train, val, test):\n",
    "        \"\"\"Abstract method to initialize X_train, y_train, X_val, y_val, X_test, y_test\"\"\"\n",
    "        pass\n",
    "\n",
    "    def scale_data(self, verbose=True):\n",
    "        \"\"\"\n",
    "        Scales the training data using the set scaler.\n",
    "        NOTE: this converts pandas dataframes into numpy ndarrays.\n",
    "        Tensorflow requires numpy ndarrays so this is intended behavior\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        verbose : bool, optional\n",
    "            If True, prints status messages. Default is True.\n",
    "\n",
    "        Returns:\n",
    "        ---------\n",
    "        Nothing, modifies in place\n",
    "        \"\"\"        \n",
    "\n",
    "        if not hasattr(self, \"X_train\"):\n",
    "            raise AttributeError(\"No X_train within object. Run train_test_split first. This is to avoid fitting the scaler with prediction data.\")\n",
    "        if verbose:\n",
    "            print(f\"Scaling training data with scaler {self.scaler}, fitting on X_train\")\n",
    "\n",
    "        # Fit scaler on row-joined training data\n",
    "        self.scaler.fit(self.X_train)\n",
    "        # Transform data using fitted scaler\n",
    "        self.X_train = self.scaler.transform(self.X_train)\n",
    "        if hasattr(self, 'X_val'):\n",
    "            if self.X_val is not None:\n",
    "                self.X_val = self.scaler.transform(self.X_val)\n",
    "        if self.X_test is not None:\n",
    "            self.X_test = self.scaler.transform(self.X_test)    \n",
    "\n",
    "    def inverse_scale(self, save_changes=False, verbose=True):\n",
    "        \"\"\"\n",
    "        Inversely scales the data to its original form. Either save changes internally,\n",
    "        or return tuple X_train, X_val, X_test\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        return_X : str, optional\n",
    "            Specifies what data to return after inverse scaling. Default is 'all_hours'.\n",
    "        save_changes : bool, optional\n",
    "            If True, updates the internal data with the inversely scaled values. Default is False.\n",
    "        verbose : bool, optional\n",
    "            If True, prints status messages. Default is True.\n",
    "        \"\"\"        \n",
    "        if verbose:\n",
    "            print(\"Inverse scaling data...\")\n",
    "        X_train = self.scaler.inverse_transform(self.X_train)\n",
    "        X_val = self.scaler.inverse_transform(self.X_val)\n",
    "        X_test = self.scaler.inverse_transform(self.X_test)\n",
    "\n",
    "        if save_changes:\n",
    "            print(\"Inverse transformed data saved\")\n",
    "            self.X_train = X_train\n",
    "            self.X_val = X_val\n",
    "            self.X_test = X_test\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Inverse scaled, but internal data not changed.\")\n",
    "            return X_train, X_val, X_test    \n",
    "    \n",
    "    def print_hashes(self, attrs_to_check = ['X_train', 'y_train', 'X_val', 'y_val', 'X_test', 'y_test']):\n",
    "        \"\"\"\n",
    "        Prints the hash of specified data attributes.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        attrs_to_check : list, optional\n",
    "            A list of attribute names to hash and print. Default includes 'X', 'y', and split data.\n",
    "        \"\"\"\n",
    "        \n",
    "        for attr in attrs_to_check:\n",
    "            if hasattr(self, attr):\n",
    "                value = getattr(self, attr)\n",
    "                print(f\"Hash of {attr}: {hash_ndarray(value)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3229d7-a801-4c4d-a6c8-0462b89af99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticMLData(MLData):\n",
    "    \"\"\"\n",
    "    Custom class to handle data scaling and extracting from dictionaries. \n",
    "    Static combines all data in train/val/test as independent observations in time. \n",
    "    So timeseries are not maintained and a single \"sample\" is one hour of data\n",
    "    Inherits from MLData class.\n",
    "    \"\"\"    \n",
    "    def _setup_data(self, train, val, test, y_col=\"fm\", verbose=True):\n",
    "        \"\"\"\n",
    "        Combines all DataFrames under 'data' keys for train, val, and test. \n",
    "        Static data does not keep track of timeseries, and throws all instantaneous samples into the same pool\n",
    "        If train and val are None, still create those names as None objects\n",
    "\n",
    "        Creates numpy ndarrays X_train, y_train, X_val, y_val, X_test, y_test\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"Subsetting input data to {self.features_list}\")\n",
    "\n",
    "        \n",
    "        X_train = self._combine_data(train)\n",
    "        self.y_train = X_train[y_col].to_numpy()\n",
    "        self.X_train = X_train[self.features_list].to_numpy()\n",
    "\n",
    "        self.X_val, self.y_val = (None, None)\n",
    "        if val:\n",
    "            X_val = self._combine_data(val)\n",
    "            self.y_val = X_val[y_col].to_numpy()\n",
    "            self.X_val = X_val[self.features_list].to_numpy()\n",
    "    \n",
    "        self.X_test, self.y_test = (None, None)\n",
    "        if test:\n",
    "            X_test = self._combine_data(test)\n",
    "            self.y_test = X_test[y_col].to_numpy()\n",
    "            self.X_test = X_test[self.features_list].to_numpy()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"X_train shape: {self.X_train.shape}, y_train shape: {self.y_train.shape}\")\n",
    "            if self.X_val is not None:\n",
    "                print(f\"X_val shape: {self.X_val.shape}, y_val shape: {self.y_val.shape}\")\n",
    "            if self.X_test is not None:\n",
    "                print(f\"X_test shape: {self.X_test.shape}, y_test shape: {self.y_test.shape}\")\n",
    "            \n",
    "    def _combine_data(self, data_dict):\n",
    "        \"\"\"Combines all DataFrames under 'data' keys into a single DataFrame.\"\"\"\n",
    "        return pd.concat([v[\"data\"] for v in data_dict.values()], ignore_index=True)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c9ce42-0800-41b8-aafb-bfea2112c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = StaticMLData(train, val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c05855-b0d5-41ef-bddd-2b6417cc8659",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9bac0c-f591-4211-b8fc-facad33bef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.X_train[:, 0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c32b94-a30d-4844-8ab0-4e1e131cbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.scale_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d25ecf-832b-400c-908d-1cd3454f10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.X_train[:, 0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165061c0-02d8-4731-8e91-7007de00d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.print_hashes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502a5a8-7684-474b-96b1-8db68b3cbc8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffaf020-bf3a-4ea6-857b-9a9632eeed88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ba673-4c22-46e7-8531-2e84abb159b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3210a69-cd3d-40e9-b84c-b49f9dfc12e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4cdd05-64a7-4124-85e6-04e08117372e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
