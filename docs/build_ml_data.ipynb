{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f4cc18-d23f-48ea-84dc-b0328a65e8e5",
   "metadata": {},
   "source": [
    "# Build Machine Learning Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79db2ac-aa72-4e5f-85e0-e21def49917d",
   "metadata": {},
   "source": [
    "The data retrieval process loops through a range of dates, retrieves and joins RAWS, HRRR, and other data sources and saves to a local directory.\n",
    "\n",
    "This notebook describes the process of reading that data, performing the final set of quality control filters, and formatting into data that can be fed into the various models used in this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed774d-7bef-48e7-b9a6-8f4ba4e17d81",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b32fd-9d6b-4582-b724-4d2a094a6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from datetime import datetime, timezone\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import synoptic\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "sys.path.append('../src')\n",
    "import reproducibility\n",
    "from utils import Dict, read_yml, read_pkl, str2time, print_dict_summary, time_range, rename_dict\n",
    "import models.moisture_models as mm\n",
    "import models.moisture_rnn as mrnn\n",
    "from models.moisture_models import XGB, LM\n",
    "import ingest.RAWS as rr\n",
    "import ingest.HRRR as ih\n",
    "import data_funcs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba6476-fbc3-4dff-93e3-ab0550a3ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"2023-01-01T00:00:00Z\"\n",
    "end = \"2023-01-06T23:00:00Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bffca74-8c6d-45b0-9726-cd2a0687a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_data = Dict(read_yml(\"../etc/params_data.yaml\"))\n",
    "print_dict_summary(params_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05eb21-3b84-4de1-96a9-40ee2826b851",
   "metadata": {},
   "source": [
    "## Retrieve Data\n",
    "\n",
    "Nested dictionary with top level key corresponding to a RAWS and subkeys for RAWS, atmospheric data (HRRR), geographic info, etc\n",
    "\n",
    "This format is used because different FMC models used in this project require different data formatting. The ODE+KF physics-based model is run pointwise and does not incorporate info from other locations. The static ML models have the least restrictive input data structure, and all observations can be thrown into one set of tabular data. The RNN models require structuring input data with the format (batch_size, timesteps, features). Thus, it is simpler to keep all data separate at separate locations and recombine in various ways at the modeling step. Also, data filters for suspect RAWS sensors are applied in the next step. This is because the raw data retrieval should not depend on hyperparameter choices related to data filters, so it is easier to collect everything and apply filters later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0547a5-0e20-4deb-a42b-524d78315726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = [\"../data/rocky_fmda/202301/fmda_20230101.pkl\", \n",
    "#          \"../data/rocky_fmda/202301/fmda_20230102.pkl\",\n",
    "#          \"../data/rocky_fmda/202301/fmda_20230103.pkl\",\n",
    "#          \"../data/rocky_fmda/202301/fmda_20230104.pkl\",\n",
    "#          \"../data/rocky_fmda/202301/fmda_20230105.pkl\",\n",
    "#          \"../data/rocky_fmda/202301/fmda_20230106.pkl\"\n",
    "#         ]\n",
    "paths = data_funcs.sort_files_by_date(\"../data/rocky_fmda/202301\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b41eb6-099b-453a-9afa-8547ddd5966c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raws_dict = data_funcs.combine_fmda_files(paths, save_path=\"../data/test_data/test_fmda_combined.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11870378-f722-423e-9ab7-82ca9dfdad2e",
   "metadata": {},
   "source": [
    "## Build ML Dataset\n",
    "\n",
    "Filter data and merge RAWS and HRRR and other sources. The file `etc/params_data.yaml` has hyperparameters related to filtering data. The steps include:\n",
    "\n",
    "- Determine atmospheric data source. Intended to be \"HRRR\" for production, but \"RAWS\" used for research purposes.\n",
    "- Combine atmospheric data predictors with FMC\n",
    "- Break timeseries into 72 hour periods, adding a column \"st_period\" starting at 0 (see README for info on why 72)\n",
    "- Apply data filters to 72 hour periods to RAWS data and remove from samples. HRRR data should already be QC'ed, so filtering will not be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176544c-e363-44c9-baf1-732e09067a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b58a8-6f41-4ade-9a3e-b92d4f789b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ml_dict = data_funcs.build_ml_data(raws_dict, hours=params_data.hours, \n",
    "                                   max_linear_time = params_data.max_linear_time, \n",
    "                                   save_path = \"../data/test_data/test_ml_dat.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2134f539-6cb4-4015-bd12-e250111fd78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raws_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722c8ac-3e73-437c-a014-5995a0fa2cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ml_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339e91d-6c3e-4134-ba22-651c4a3ee790",
   "metadata": {},
   "source": [
    "## Setup CV\n",
    "\n",
    "Steps:\n",
    "* Determine time ranges for train/val/test\n",
    "* Get stations with data availability in those periods\n",
    "* Sample stations for train/val/test\n",
    "\n",
    "\n",
    "Different stations will have different gaps of data availability for the train/val/test time periods. When selecting stations for inclusion in those periods, we use the following methodology:\n",
    "* Let $N$ be the total number of stations that returned data over the combined train/val/test times\n",
    "* Let $N_t$ be the number of stations included in each of the validation and test sets, and are chosen to be the nearest integer to 10\\% of $N$\n",
    "* Starting with the test time period, we select $N_t$ of the number of stations with data availability in the period. In other words, there may be less than $N$ stations with data availability in the test period, but we select $N_t$ if possible\n",
    "* Then, we select $N_t$ stations for inclusion in the validation set, excluding any of the $N_t$ stations included in the test set\n",
    "* Finally, we use any remaining stations for the training set that weren't included in either of the validation or test sets. So there is a maximum of $N-2\\cdot N_t$ stations included in the training set \n",
    "\n",
    "This methodology makes it so the number of stations included in the training set varies and is sometimes less than $N-2\\cdot N_t$. We fix the number of stations in the test and validation sets and allow the number of stations in the training set to vary. This is because we don't want accuracy metrics to be calculated consistently for those periods. If there are fewer stations with data availability for a certain period, we want that be to reflected in a smaller training set and presumably less accurate metrics on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8357373-00b7-44cb-a91e-b79fe8319e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_times, val_times, test_times = data_funcs.cv_time_setup(\"2023-01-29T00:00:00Z\", \n",
    "                                                train_hours=24*28, forecast_hours=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff1767-11c3-46da-9b44-7032662660ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_sts, val_sts, te_sts = data_funcs.cv_space_setup(ml_dict, \n",
    "                                                    val_times=val_times, \n",
    "                                                    test_times=test_times, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fd15e-5087-47d7-80f2-681bd0820eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85a83f-091f-41b3-9536-fb5366eb0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(te_sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68109f3c-2248-4360-ad46-e11a98f4389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_funcs.get_sts_and_times(ml_dict, tr_sts, train_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d75b9-d169-471e-b616-f4138634f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = data_funcs.get_sts_and_times(ml_dict, val_sts, val_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be4977-e533-4bf2-b3d2-f60f02d53f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_funcs.get_sts_and_times(ml_dict, te_sts, test_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568c10d7-fc06-4c89-aa22-8f6e4de7519d",
   "metadata": {},
   "source": [
    "## ODE+KF Data\n",
    "\n",
    "* Run on 72 hour stretches (24 spinup, 48 val)\n",
    "* Get test station list used by other models\n",
    "* For those test stations, use `get_sts_and_times` accounting for the spinup period\n",
    "    * So adjust test times by subtracting 24 hours to account for spinup\n",
    " \n",
    "Function `get_ode_data` wraps the `get_sts_and_times` function... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c58b72-feb2-4892-973b-7fb7eef71be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_data = data_funcs.get_ode_data(ml_dict, te_sts, test_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766c087-84ef-46b6-8dd8-95c5ff5e1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ode = mm.ODE_FMC()\n",
    "m, errs = ode.run_model(ode_data, hours=72, h2=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149cf12c-db9a-4f32-8070-cf4bd3c9cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE Over Test Period: {errs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc3c8f-8789-4d9c-a53f-0eabc7b88fd2",
   "metadata": {},
   "source": [
    "## Static ML Data\n",
    "\n",
    "Throw all train/val/test data together without worrying about timesteps samples. In other words, data can all be jumbled up in any order as observations are considered independent in time.\n",
    "\n",
    "Data is stored as a custom class `StaticMLData` defined in `models/moisture_models.py`. A custom class is used to organize data scaling and inverse scaling. A scaler should be fit using only the training data, and then applied to the val and test data to avoid data leakage. This is done internally in the StaticMLData class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c9ce42-0800-41b8-aafb-bfea2112c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = data_funcs.StaticMLData(train, val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c32b94-a30d-4844-8ab0-4e1e131cbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.scale_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffaf020-bf3a-4ea6-857b-9a9632eeed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, v, te = dat.inverse_scale(save_changes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb8d42-d1d4-4753-b381-173dc8ba7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dat.X_train[:, 0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15177e-8196-43ea-adc1-fe2eef16fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr[:, 0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ead97-2a25-4087-a35a-70320a7e0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.scale_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066d23b-0123-4cb9-b268-d06989d6a06c",
   "metadata": {},
   "source": [
    "### Fitting Static Models\n",
    "\n",
    "Using StaticMLData custom class above, fit and predict using some static ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb84ff9-9af4-4ff0-abc9-644f9f65a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility.set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3d867-b34e-4b4a-b137-2b8c8a089ac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_model = XGB(mm.xgb_params, random_state=42)\n",
    "m, err = xgb_model.run_model(dat)\n",
    "print(f\"XGBoost Test RMSE: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3986a3-624c-45c8-a020-1ef5d3c52bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model = LM(mm.lm_params)\n",
    "m, err = lm_model.run_model(dat)\n",
    "print(f\"LM Test RMSE: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c14999-9b74-4faf-8eed-0a5318826b05",
   "metadata": {},
   "source": [
    "## RNN Data\n",
    "\n",
    "For training RNNs (simple, LSTM, GRU included), the data must be structured as `(batch_size, timesteps, features)`. So a single \"sample\" in this context is a timeseries of length `timesteps` and dimensionality `features`. RNNs can be trained with different size timesteps and batch sizes, which is often useful in the context of natural language processing. However, if running an RNN in \"stateful\" model, which maintains the dependence between different samples from the same location, the data must have consistent number of timesteps and batch size across all inputs. Further, when using static features like lon/lat or elevation, it is desirable to have samples from different locations within the same batch. Otherwise, if a batch is constructed with samples all from the same location, the static features will have zero variance for a given batch and the model cannot learn any relationship between the static features and the outcome variable for that batch.\n",
    "\n",
    "Data is stored in a custom class `RNNData` defined in `models/moisture_rnn.py`. A custom class is used to organize scaling as well as batch construction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3df6f7-9f67-46fc-ab0b-a6df66fb4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = mrnn.RNNData(train, val, test,\n",
    "                  method=\"random\", random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812c8a6-e2b2-48e8-a3e3-b2dbd639eae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978b5f8-76a6-4cde-884d-3bd772ae90b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d4126-300b-4020-ae82-6c5c096d404a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e7add-dfe3-4c1f-872a-1825037dac1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce074ab-ae4c-4052-b7cf-67af64033977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e3e0c-f78d-4475-a3f8-10be08b92de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3c5da-e903-4163-99c5-cba026fe2d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff9187-e82f-457d-b810-1d3668ccf1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae365e-4ba1-46ac-bbc6-e1573e5090ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee32918-93fe-4820-98ea-4be4780dc850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a72acd8-a33e-4a6d-9972-4f96ffabbbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591edd8-4b81-4c63-9e6b-aa4708ab0012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
