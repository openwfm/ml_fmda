{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f4cc18-d23f-48ea-84dc-b0328a65e8e5",
   "metadata": {},
   "source": [
    "# Build Machine Learning Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79db2ac-aa72-4e5f-85e0-e21def49917d",
   "metadata": {},
   "source": [
    "The data retrieval process loops through a range of dates, retrieves and joins RAWS, HRRR, and other data sources and saves to a local directory.\n",
    "\n",
    "This notebook describes the process of reading that data, performing the final set of quality control filters, and formatting into data that can be fed into the various models used in this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed774d-7bef-48e7-b9a6-8f4ba4e17d81",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509b32fd-9d6b-4582-b724-4d2a094a6832",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mingest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mRAWS\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrr\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mingest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mHRRR\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mih\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata_funcs\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/Wildfire/ml_fmda/docs/../src/data_funcs.py:312\u001b[0m\n\u001b[1;32m    308\u001b[0m     ode_data \u001b[38;5;241m=\u001b[39m get_sts_and_times(d, sts, all_times)\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ode_data\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMLData\u001b[39;00m(ABC):\n\u001b[1;32m    313\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    Abstract base class for ML Data, providing support for scaling. \u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    Scaling performed on training data and applied to val and test.\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m    \n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, train, val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, scaler\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m\"\u001b[39m, features_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documents/Projects/Wildfire/ml_fmda/docs/../src/data_funcs.py:329\u001b[0m, in \u001b[0;36mMLData\u001b[0;34m()\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# Assuming that units are all the same as it was checked in a previous step\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train\u001b[38;5;241m.\u001b[39mvalues()))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 329\u001b[0m f\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_checks\u001b[39m(\u001b[38;5;28mself\u001b[39m, train, val, test, scaler):\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validates input types for train, val, test, and scaler.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "from datetime import datetime, timezone\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import synoptic\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "sys.path.append('../src')\n",
    "from utils import Dict, read_yml, read_pkl, str2time, print_dict_summary, time_range, rename_dict\n",
    "import models.moisture_models as mm\n",
    "from models.moisture_models import XGB, LM\n",
    "import ingest.RAWS as rr\n",
    "import ingest.HRRR as ih\n",
    "import data_funcs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba6476-fbc3-4dff-93e3-ab0550a3ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"2023-01-01T00:00:00Z\"\n",
    "end = \"2023-01-06T23:00:00Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bffca74-8c6d-45b0-9726-cd2a0687a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_data = Dict(read_yml(\"../etc/params_data.yaml\"))\n",
    "print_dict_summary(params_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05eb21-3b84-4de1-96a9-40ee2826b851",
   "metadata": {},
   "source": [
    "## Retrieve Data\n",
    "\n",
    "Nested dictionary with top level key corresponding to a RAWS and subkeys for RAWS, atmospheric data (HRRR), geographic info, etc\n",
    "\n",
    "This format is used because different FMC models used in this project require different data formatting. The ODE+KF physics-based model is run pointwise and does not incorporate info from other locations. The static ML models have the least restrictive input data structure, and all observations can be thrown into one set of tabular data. The RNN models require structuring input data with the format (batch_size, timesteps, features). Thus, it is simpler to keep all data separate at separate locations and recombine in various ways at the modeling step. Also, data filters for suspect RAWS sensors are applied in the next step. This is because the raw data retrieval should not depend on hyperparameter choices related to data filters, so it is easier to collect everything and apply filters later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0547a5-0e20-4deb-a42b-524d78315726",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"../data/rocky_fmda/202301/fmda_20230101.pkl\", \n",
    "         \"../data/rocky_fmda/202301/fmda_20230102.pkl\",\n",
    "         \"../data/rocky_fmda/202301/fmda_20230103.pkl\",\n",
    "         \"../data/rocky_fmda/202301/fmda_20230104.pkl\",\n",
    "         \"../data/rocky_fmda/202301/fmda_20230105.pkl\",\n",
    "         \"../data/rocky_fmda/202301/fmda_20230106.pkl\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62448d6-7294-4cbd-9570-978c04635e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data_funcs\n",
    "importlib.reload(data_funcs)\n",
    "from data_funcs import combine_fmda_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b41eb6-099b-453a-9afa-8547ddd5966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws_dict = data_funcs.combine_fmda_files(paths, save_path=\"../data/test_data/test_fmda_combined.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11870378-f722-423e-9ab7-82ca9dfdad2e",
   "metadata": {},
   "source": [
    "## Build ML Dataset\n",
    "\n",
    "Filter data and merge RAWS and HRRR and other sources. The file `etc/params_data.yaml` has hyperparameters related to filtering data. The steps include:\n",
    "\n",
    "- Determine atmospheric data source. Intended to be \"HRRR\" for production, but \"RAWS\" used for research purposes.\n",
    "- Combine atmospheric data predictors with FMC\n",
    "- Break timeseries into 72 hour periods, adding a column \"st_period\" starting at 0 (see README for info on why 72)\n",
    "- Apply data filters to 72 hour periods to RAWS data and remove from samples. HRRR data should already be QC'ed, so filtering will not be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176544c-e363-44c9-baf1-732e09067a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b58a8-6f41-4ade-9a3e-b92d4f789b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ml_dict = data_funcs.build_ml_data(raws_dict, hours=params_data.hours, \n",
    "                                   max_linear_time = params_data.max_linear_time, \n",
    "                                   save_path = \"../data/test_data/test_ml_dat.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2134f539-6cb4-4015-bd12-e250111fd78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raws_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722c8ac-3e73-437c-a014-5995a0fa2cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ml_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437d6d3-6b96-48a0-ade3-6c77f4aa383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import hash_ndarray\n",
    "hash_ndarray(ml_dict[\"RFRC2\"][\"data\"][\"fm\"].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339e91d-6c3e-4134-ba22-651c4a3ee790",
   "metadata": {},
   "source": [
    "## Setup CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8357373-00b7-44cb-a91e-b79fe8319e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_times, val_times, test_times = data_funcs.cv_time_setup(\"2023-01-05T00:00:00Z\", \n",
    "                                                train_hours=48*2, forecast_hours=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff1767-11c3-46da-9b44-7032662660ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "stids = [*ml_dict.keys()]\n",
    "\n",
    "tr_sts, val_sts, te_sts = data_funcs.cv_space_setup(stids, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fd15e-5087-47d7-80f2-681bd0820eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85a83f-091f-41b3-9536-fb5366eb0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(te_sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68109f3c-2248-4360-ad46-e11a98f4389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_funcs.get_sts_and_times(ml_dict, tr_sts, train_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d75b9-d169-471e-b616-f4138634f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = data_funcs.get_sts_and_times(ml_dict, val_sts, val_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be4977-e533-4bf2-b3d2-f60f02d53f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_funcs.get_sts_and_times(ml_dict, te_sts, test_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568c10d7-fc06-4c89-aa22-8f6e4de7519d",
   "metadata": {},
   "source": [
    "## ODE+KF Data\n",
    "\n",
    "* Run on 72 hour stretches (24 spinup, 48 val)\n",
    "* Get test station list used by other models\n",
    "* For those test stations, use `get_sts_and_times` accounting for the spinup period\n",
    "    * So adjust test times by subtracting 24 hours to account for spinup\n",
    " \n",
    "Function `get_ode_data` wraps the `get_sts_and_times` function... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c58b72-feb2-4892-973b-7fb7eef71be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_data = data_funcs.get_ode_data(ml_dict, te_sts, test_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766c087-84ef-46b6-8dd8-95c5ff5e1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ode = mm.ODE_FMC()\n",
    "m, errs = ode.run_model(ode_data, hours=72, h2=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149cf12c-db9a-4f32-8070-cf4bd3c9cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE Over Test Period: {errs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba43ee3-1fda-48bd-9198-e2aa5cb607ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cabc3c8f-8789-4d9c-a53f-0eabc7b88fd2",
   "metadata": {},
   "source": [
    "## Static ML Data\n",
    "\n",
    "Throw all train/val/test data together without worrying about timesteps samples. In other words, data can all be jumbled up in any order as observations are considered independent in time.\n",
    "\n",
    "Data is stored as a custom class `StaticMLData` defined in `models/moisture_models.py`. A custom class is used to organize data scaling and inverse scaling. A scaler should be fit using only the training data, and then applied to the val and test data to avoid data leakage. This is done internally in the StaticMLData class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c9ce42-0800-41b8-aafb-bfea2112c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = data_funcs.StaticMLData(train, val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e19d7-4962-4718-a645-9d1dee8fc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dat.X_train[:, 0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c32b94-a30d-4844-8ab0-4e1e131cbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.scale_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffaf020-bf3a-4ea6-857b-9a9632eeed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, v, te = dat.inverse_scale(save_changes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb8d42-d1d4-4753-b381-173dc8ba7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dat.X_train[:, 0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15177e-8196-43ea-adc1-fe2eef16fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr[:, 0].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066d23b-0123-4cb9-b268-d06989d6a06c",
   "metadata": {},
   "source": [
    "### Fitting Static Models\n",
    "\n",
    "Using StaticMLData custom class above, fit and predict using some static ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3d867-b34e-4b4a-b137-2b8c8a089ac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_model = XGB(mm.xgb_params)\n",
    "m, err = xgb_model.run_model(dat)\n",
    "print(f\"XGBoost Test RMSE: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3986a3-624c-45c8-a020-1ef5d3c52bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model = LM(mm.lm_params)\n",
    "m, err = lm_model.run_model(dat)\n",
    "print(f\"LM Test RMSE: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c14999-9b74-4faf-8eed-0a5318826b05",
   "metadata": {},
   "source": [
    "## RNN Data\n",
    "\n",
    "For training RNNs (simple, LSTM, GRU included), the data must be structured as `(batch_size, timesteps, features)`. So a single \"sample\" in this context is a timeseries of length `timesteps` and dimensionality `features`. RNNs can be trained with different size timesteps and batch sizes, which is often useful in the context of natural language processing. However, if running an RNN in \"stateful\" model, which maintains the dependence between different samples from the same location, the data must have consistent number of timesteps and batch size across all inputs. Further, when using static features like lon/lat or elevation, it is desirable to have samples from different locations within the same batch. Otherwise, if a batch is constructed with samples all from the same location, the static features will have zero variance for a given batch and the model cannot learn any relationship between the static features and the outcome variable for that batch.\n",
    "\n",
    "Data is stored in a custom class `RNNData` defined in `models/moisture_rnn.py`. A custom class is used to organize scaling as well as batch construction. \n",
    "\n",
    "Steps:\n",
    "* Remove data from train/test/val shorter than needed length of timeseries\n",
    "    * For non-stateful models, sequences of data must be greater than or equal to the `timesteps` hyperparameter. (So for a given loc, we can have stretches of data of length timesteps over any period defined as the train times)\n",
    "    * For stateful models, we need continuity across samples. We therefore discard any locations where the obsevations are not continuous over a certain length of time. (So for a given loc, we need samples of length `timesteps` that line up in time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768380a9-3503-4c98-bbf7-cc6973830f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [\"Ed\", \"Ew\", \"rain\"]\n",
    "df = train[\"AENC2\"][\"data\"][features_list]\n",
    "df[\"ind\"] = np.arange(0, len(df.copy()))\n",
    "X = df.to_numpy()\n",
    "y = train[\"AENC2\"][\"data\"][\"fm\"].to_numpy().reshape(-1, 1)\n",
    "times = train[\"AENC2\"][\"times\"]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e1a42-97c1-4661-92d6-71d76967e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import is_consecutive_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf1170-62d9-49a3-ac9b-e8688a9e61e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_consecutive_hours(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8649860b-0858-4c10-94b2-6de5ea1e3c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def staircase(X, y, timesteps=12, method=\"consecutive\", return_sequences=False, verbose=True):\n",
    "    \"\"\"\n",
    "    NON-STATEFUL method. For given cases in input data dictionary, \n",
    "    extract samples of length `timesteps` and data of shorter length.\n",
    "    Non-stateful since samples of length timesteps for a given location\n",
    "    need not be directly ordered in time. Allows for getting samples of \n",
    "    length timesteps for a given location over any time within train/val/test \n",
    "    window.\n",
    "\n",
    "    If consecutive, samples of length timesteps taken in order, so no overlap\n",
    "        num samples with be total_times // timesteps\n",
    "    If sliding, samples of length timesteps taken while shifting one step, so lots of overlap\n",
    "        num samples will be total_times - timesteps + 1\n",
    "\n",
    "    Args: \n",
    "        - X: numpy ndarray of dims (total_times, n_features)\n",
    "        - method: (str) one of \"sliding\" or consecutive\n",
    "        - return_sequences: (bool) whether to return the entire sequence of y values for the sample or only the\n",
    "                            last time. If False, y has dims (n_samples, 1). If True, (n_samples, timesteps)\n",
    "    Returns: \n",
    "        - X_samples: either shape (total_times // timesteps, timesteps, n_features) OR (total_times - timesteps + 1, timesteps, n_features) \n",
    "        depending on consecutive or sliding method\n",
    "        - y_samples: Either shape (num_samples, 1) or (num_samples, timesteps), \n",
    "        where num_samples determined by X shape\n",
    "    \"\"\"\n",
    "    \n",
    "    total_times, features = X.shape\n",
    "    \n",
    "    if method == \"sliding\":\n",
    "        nsamples = total_times - timesteps + 1\n",
    "        X_samples = np.lib.stride_tricks.sliding_window_view(X, (timesteps, features)).squeeze(axis=1)\n",
    "        y_samples = np.lib.stride_tricks.sliding_window_view(y.squeeze(), timesteps)\n",
    "\n",
    "\n",
    "    elif method == \"consecutive\":\n",
    "        nsamples = total_times // timesteps  # Only full batches\n",
    "        X_samples = X[:nsamples * timesteps].reshape(nsamples, timesteps, features)\n",
    "        y_samples = y[:nsamples * timesteps].reshape(nsamples, timesteps)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be either 'consecutive' or 'sliding'.\")\n",
    "\n",
    "    if not return_sequences:\n",
    "        y_samples = y_samples[:, -1].reshape(-1, 1)  # Keep only the last timestep\n",
    "    \n",
    "    if verbose:\n",
    "        print('staircase: shape X = ',X_samples.shape)\n",
    "        print('staircase: shape y = ',y_samples.shape)\n",
    "        print('staircase: timesteps=',timesteps)\n",
    "        print('staircase: return_sequences=',return_sequences)        \n",
    "\n",
    "    return X_samples, y_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b61f94-1767-4285-977f-d66ee150bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = staircase(X, y, timesteps=12, method=\"consecutive\", return_sequences=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1eedb7-1afa-4f6b-adf5-b9412e0c5aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67092a-2961-47b7-b2f9-308d4b6e7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb34474-abdc-49eb-81d2-30c96cf3413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, y2 = staircase(X, y, timesteps=12, method=\"sliding\", return_sequences=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7644aa8-a97f-49ff-9db8-9e315094b7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6adf14-59ed-4c0f-9031-92d9d05074fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e46813-6efb-45e2-80b0-e27d7eebcaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [\"Ed\", \"Ew\", \"rain\"]\n",
    "y_col=\"fm\"\n",
    "hours = 36\n",
    "# y_list = [d[\"data\"][y_col].values for d in train.values()]\n",
    "# X_list = [d[\"data\"][features_list].values for d in train.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932715b-1d8b-4838-bd25-3c4e4aa74d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lists of X, y and times\n",
    "X_list = [d[\"data\"].values for d in train.values()]\n",
    "y_list = [d[\"data\"][y_col].values for d in train.values()]\n",
    "times_list = [d[\"times\"] for d in train.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699c2ba-8b4c-4197-8ba4-4c814440421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import is_consecutive_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd5f58-a0d2-4791-b92e-6323501aac23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5667527-7f10-4279-8ff8-76e8e74cb2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3385f-e445-4972-9105-92f9686c21fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd97a9-b608-4e20-b6d0-53888b166939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def build_training_batches(X_list, y_list, batch_size, timesteps=12, hours = 36,\n",
    "                           method=\"consecutive\", return_sequences=False, start_times=\"zeros\", verbose=True):\n",
    "    \"\"\"\n",
    "    Construct data for RNN training (and validation data) with format (batch_size, timesteps, features) \n",
    "    Runs staircase with given params, then interlaces the data so that a single batch has samples from different\n",
    "    locations and thus can learn relationships for features that are static for a given location\n",
    "\n",
    "    Args:\n",
    "        - X_list: (list) list of numpy ndarrays of predictors\n",
    "        - y_list: (list) list of numpy ndarrays of response data\n",
    "        - batch_size: (int) number of samples of length timesteps to include in a single iteration of weight updates\n",
    "        - timesteps: (int) number of discrete time steps that defines a single sample\n",
    "        - hours: (bool) Number of hours to . Any set of samples less than hours will be discarded. \n",
    "            For stateful structure, hidden state will be maintained for number of samples N where N*timesteps=hours\n",
    "            NOTE, hours should be divisible by timesteps\n",
    "            If Non-stateful structure, set hours equal to timesteps\n",
    "        - method: one of \"consecutive\" or \"sliding\"\n",
    "        - start_times: if \"zeros\" all samples start at time 0. (Only one for now)\n",
    "    Returns:\n",
    "        XX, yy: tuple of structured predictors and outcomes variables. \n",
    "            XX shape will be (num_samples, timesteps, features), where num_samples determined by batch size and input X length\n",
    "            yy shape will be (num_samples, 1) OR (num_samples, timesteps) if return sequences\n",
    "    \"\"\"\n",
    "\n",
    "    if method != \"consecutive\":\n",
    "        raise ValueError(\"Only method=consecutive is implemented yet\")\n",
    "    if hours % timesteps != 0:\n",
    "        warnings.warn(f\"Input hours {hours} not divisible by input timesteps {timesteps}, may lead to unexpected behavior\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f183192-07ea-4ee5-a075-ef8a2ad20d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply staircase to each list element\n",
    "X_samples_list, y_samples_list = zip(*[staircase(X, y, timesteps=12, method=\"consecutive\", verbose=False, return_sequences=False) for X, y in zip(X_list, y_list)])\n",
    "X_samples_list, y_samples_list = list(X_samples_list), list(y_samples_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742fb60-f8c1-4969-b507-ffae58f7f847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24253522-4930-487c-b815-4e7bf79a08f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d193c1-1df8-4790-ba2d-6dbaaaeba05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd71f16-2f4c-40f0-a41b-369a7f476ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f4c6f-b1bc-4c4c-b236-0cc019c1f356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d30e9-015b-4dad-8bd7-fa13b0da8b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f923d8f2-0510-42b2-a5d0-632475033a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ea7e5-1fae-4377-ade8-95f142d33e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_funcs import MLData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22988dc0-7a7b-42b2-aef5-45bd9b5c0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNData(MLData):\n",
    "    \"\"\"\n",
    "    Custom class to handle RNN data. Performs data scaling and stateful batch structuring.\n",
    "    In this context, a single \"sample\" from RNNData is a timeseries with dimensionality (timesteps, n_features)\n",
    "    \"\"\"\n",
    "    def _setup_data(self, train, val, test, y_col=\"fm\", verbose=True):\n",
    "        \"\"\"\n",
    "        Combines DataFrames under 'data' keys for train, val, and test. \n",
    "        Batch structure using staircase functions.\n",
    "\n",
    "        Creates numpy ndarrays X_train, y_train, X_val, y_val, X_test, y_test\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"Subsetting input data to {self.features_list}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8e085-454c-4298-8d41-9224c9ef8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dict[\"RLAS2\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a5b2f-611c-4110-bd11-79bd02eee99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dict[\"RLAS2\"][\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec81ee2-9d2a-46e7-8e2e-a446466599a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [\"Ed\", \"Ew\", \"rain\"]\n",
    "y_col=\"fm\"\n",
    "y_list = [d[\"data\"][y_col].values for d in train.values()]\n",
    "X_list = [d[\"data\"][features_list].values for d in train.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e96b4f-a905-4fea-9add-97dff18e88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b921d91-5486-4a3c-8b0a-b013a589bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4cdd05-64a7-4124-85e6-04e08117372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7b230-27e5-4ec7-8c9e-26d33bf5e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ec29d-3be4-4c07-8862-9f6699ed36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7044f4-c152-4e49-84af-f233e14adb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import models.moisture_rnn\n",
    "importlib.reload(models.moisture_rnn)\n",
    "from models.moisture_rnn import staircase_spatial, batch_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48064ed8-50d4-49f3-91d5-85007aaa96f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, yy, n_seqs = staircase_spatial(\n",
    "    X_list, y_list, batch_size = 32, timesteps=12,\n",
    "    start_times = \"zeros\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6cbc5c-9874-4aa6-a142-458f0eea6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_loc = len(y_list) # assuming each list entry for y is a separate location\n",
    "loc_ids = np.arange(n_loc)\n",
    "start_times = np.zeros(n_loc)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a140f0f-e161-4048-afc1-9ca0a02e8cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_batch, t_batch =  batch_setup(loc_ids, batch_size), batch_setup(start_times, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a7687-da81-46d5-b2f9-fcd95f2627c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loc_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b5aeef-f1b0-4c0b-b0a4-58db2eb49731",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac501df2-fa13-4c09-8acb-1e69c5486e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae2ba6-b768-4eb7-937e-083b33908ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = min(len(yi) for yi in y_list)\n",
    "print(hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcdc73b-281d-42c1-af92-d7d1ca02a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.moisture_rnn import staircase_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac6611-81ac-421c-91fd-0c55d8447cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over batches and construct with staircase_2\n",
    "Xs = []\n",
    "ys = []\n",
    "for i in range(0, len(loc_batch)):\n",
    "    locs_i = loc_batch[i]\n",
    "    ts = t_batch[i]\n",
    "    for j in range(0, len(locs_i)):\n",
    "        t0 = int(ts[j])\n",
    "        tend = t0 + hours\n",
    "        # Create RNNData Dict\n",
    "        # Subset data to given location and time from t0 to t0+hours\n",
    "        k = locs_i[j] # Used to account for fewer locations than batch size\n",
    "        X_temp = X[k][t0:tend,:]\n",
    "        y_temp = y[k][t0:tend].reshape(-1,1)\n",
    "\n",
    "        # Format sequences\n",
    "        Xi, yi = staircase_2(\n",
    "            X_temp, \n",
    "            y_temp, \n",
    "            timesteps = timesteps, \n",
    "            batch_size = 1,  # note: using 1 here to format sequences for a single location, not same as target batch size for training data\n",
    "            verbose=False)\n",
    "    \n",
    "        Xs.append(Xi)\n",
    "        ys.append(yi)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b3318-d463-4b80-849f-35b3691ce73d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e656e16-7969-466f-b8bd-ecd45658d868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae365e-4ba1-46ac-bbc6-e1573e5090ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee32918-93fe-4820-98ea-4be4780dc850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a72acd8-a33e-4a6d-9972-4f96ffabbbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591edd8-4b81-4c63-9e6b-aa4708ab0012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
