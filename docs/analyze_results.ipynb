{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f4cc18-d23f-48ea-84dc-b0328a65e8e5",
   "metadata": {},
   "source": [
    "# Analyze Project Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79db2ac-aa72-4e5f-85e0-e21def49917d",
   "metadata": {},
   "source": [
    "This notebook is indended to analyze and visualize the the accuracy of the FMC models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed774d-7bef-48e7-b9a6-8f4ba4e17d81",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b32fd-9d6b-4582-b724-4d2a094a6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "import itertools\n",
    "sys.path.append('../src')\n",
    "from utils import Dict, read_yml, read_pkl, print_dict_summary\n",
    "from data_funcs import flag_lag_stretches, get_sts_and_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0008f32b-0f6a-4def-95e2-e974b1b47b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_path = \"../data/rocky_2024_climatology_forecasts.pkl\" # climatology forecast outputs\n",
    "ml_forecast_dir = \"../outputs/forecast_analysis_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05eb21-3b84-4de1-96a9-40ee2826b851",
   "metadata": {},
   "source": [
    "### Read Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46784f-9dce-4890-9634-78d13ddbc422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract forecast periods from files\n",
    "forecast_files = os.listdir(ml_forecast_dir)\n",
    "forecast_files = [f for f in forecast_files if (f.endswith('.pkl') and not f.startswith('ml_'))]# Remove other files\n",
    "forecast_starts = np.array([datetime.strptime(f.split(\".\")[0], \"%Y%m%d_%H\") for f in forecast_files])\n",
    "forecast_files = np.array(forecast_files)[forecast_starts.argsort()]\n",
    "forecast_starts.sort()\n",
    "\n",
    "ml_data = read_pkl(osp.join(ml_forecast_dir, \"ml_data.pkl\"))\n",
    "clim = read_pkl(clim_path)\n",
    "ml_results = [read_pkl(osp.join(ml_forecast_dir, f)) for f in forecast_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece22b93-1b45-45ca-b809-d717fd6e495c",
   "metadata": {},
   "source": [
    "## Calculate Accuracy for Climatology\n",
    "\n",
    "NOTE: As of Feb 25, test forecast analysis ran in 2023, and climatology only exists for 2024. Can't combine for now\n",
    "\n",
    "The climatology method used in this project produces forcasts for all stations. Note that climatology forecasts are generated using observed data from RAWS. The ML models generate forecasts by using no observed data from the test RAWS stations. So the climatology method has an advantage relative to the ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d0678-31a5-4195-ae0f-95e07ed328a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract stations used in each forecast period\n",
    "# # Additionally, perform some checks to make sure data looks right\n",
    "# sts = []\n",
    "# for i, fperiod in enumerate(ml_results):\n",
    "#     stids = fperiod['stids']\n",
    "#     times = fperiod['times']\n",
    "#     times.sort()\n",
    "#     # Check times match, num stations matches\n",
    "#     assert pd.Timestamp(forecast_starts[i], tz=\"UTC\") == times[0], \"Time array from ML output dict doesn't match target file time\"\n",
    "#     for mod in ['RNN']:\n",
    "#         assert len(fperiod[mod]['loc_rmse']) == len(stids), \"Mismatch between number of stations and number of RMSE per station\"\n",
    "\n",
    "#     sts.append(stids)\n",
    "#     print('~'*75)\n",
    "#     print(f\"Analyzing Forecast Period {i}\")\n",
    "#     print(f\"Forecast Start Time: {times.min()}\")\n",
    "#     print(f\"Forecast End Time: {times.max()}\")    \n",
    "#     print(f\"Test Stations: {stids}\")\n",
    "\n",
    "#     # Extract test station observed FMC data\n",
    "#     obs = get_sts_and_times(ml_data, stids, times)\n",
    "#     assert [*obs.keys()] == stids, f\"Retrieved observed data from ml_data doesn't match test stids: {[*obs.keys()]}, {stids=}\"\n",
    "#     obs_fm = np.stack([v[\"data\"][\"fm\"].values[:, np.newaxis] for v in obs.values()]) # Get 3d array, (n_loc, 48, 1)\n",
    "#     assert obs_fm.shape == (len(stids), 48, 1), f\"Observed FMC data unexpected shape. Expected {(len(stids), 48, 1)}, received {obs_fm.shape}\"\n",
    "\n",
    "#     # Extract climatology forecasts for given times and stids\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b2311-29d0-4529-9622-0132fd8aa44d",
   "metadata": {},
   "source": [
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03611135-4803-49f4-9722-56ebea7073dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some checks on time and location, combine results into df\n",
    "ode_errs = []\n",
    "xgb_errs = []\n",
    "rnn_errs = []\n",
    "for i, fperiod in enumerate(ml_results):\n",
    "    stids = fperiod['stids']\n",
    "    times = fperiod['times']\n",
    "    times.sort()\n",
    "    # Check times match, num stations matches\n",
    "    assert pd.Timestamp(forecast_starts[i], tz=\"UTC\") == times[0], \"Time array from ML output dict doesn't match target file time\"\n",
    "    for mod in ['RNN']:\n",
    "        assert len(fperiod[mod]['loc_rmse']) == len(stids), \"Mismatch between number of stations and number of RMSE per station\"\n",
    "\n",
    "    ode_errs.append(fperiod['ODE']['rmse'])\n",
    "    xgb_errs.append(fperiod['XGB']['rmse'])\n",
    "    rnn_errs.append(fperiod['RNN']['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239c1dd-f363-4e8a-b4dd-5ee4ea36cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fperiod['ODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43047f60-e5d8-4257-9ac1-b5eab4d0fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'ODE': ode_errs,\n",
    "    'XGB': xgb_errs,\n",
    "    'RNN': rnn_errs,\n",
    "})\n",
    "df.index = forecast_starts\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25574e-7624-4c32-a22b-3da7d77e4ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Error for Model\n",
    "means = df.mean(axis=0)\n",
    "stds = df.std(axis=0)\n",
    "\n",
    "overall_errs_df = pd.DataFrame({\"Mean RMSE\": means, \"(Std)\": stds})\n",
    "overall_errs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275530b-c238-4a62-8a06-56634c458241",
   "metadata": {},
   "source": [
    "## T Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a82ac-0d51-43f5-b4bf-7de4c2fa0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Pairwise t-tests\n",
    "col_pairs = list(itertools.combinations(df.columns, 2))\n",
    "\n",
    "# Apply t-test to each pair\n",
    "ttests = {\n",
    "    (col1, col2): stats.ttest_rel(df[col1], df[col2])\n",
    "    for col1, col2 in col_pairs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c1f1c3-a6b3-48ed-8285-5edf30c4076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9b9fa8-a928-4d0d-9ea9-b8e36f6d6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of ttests run: {len(col_pairs)}\")\n",
    "print(f\"Bonferroni Corrected Thresholds:\")\n",
    "print(f\"    Threshold 0.05 :  Corrected {0.05/len(col_pairs)}\")\n",
    "print(f\"    Threshold 0.01 :  Corrected {0.01/len(col_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44107491-c4d6-42fd-ae24-2a893cea990d",
   "metadata": {},
   "source": [
    "## Skill Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce57426-f60d-40b1-9e0a-42a67bea1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_model = overall_errs_df.loc['RNN'].iloc[0]\n",
    "rmse_baseline1 = overall_errs_df.loc['ODE'].iloc[0]\n",
    "\n",
    "print(f\"RMSE Skill Score (ODE Baseline): \")\n",
    "print(f\"    {1-rmse_model/rmse_baseline1}\")\n",
    "print()\n",
    "print(f\"MSE Skill Score (ODE Baseline): \")\n",
    "print(f\"    {1-rmse_model**2/rmse_baseline1**2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc403ac-71d3-4dbb-a33f-5cdd79219535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd16ee7-6c1c-4f97-9b4a-1438741c9e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db5327f-dc56-4fa9-9a57-2ef0a7d4b2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d5e0c2-cb9f-48db-b0a2-c88248c9cf9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192d1ab-d8d7-4ed6-a71d-556c3666b009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
