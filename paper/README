This directory stores materials relevant to the publication *A Recurrent Neural Network for Forecasting Fuel Moisture Content with Inputs from Numerical Weather Models*, submitted to MDPI Fire on ____, 2025

Corresponding author: Jonathon Hirschi
jonathon.hirschi@ucdenver.edu

Please reach out for any assistance reproducing the original analysis.

# Setup

## Python Environments
There are two environments used in this project, `ml_fmda_data` and `ml_fmda_models`. The former has packages related to data APIs, and the latter tensorflow, keras, and other modeling software. Maintaiining a single package for the entire project proved too unstable. To build the conda envs, run:

```
conda env create -f install/env_data.yml
```

```
conda env create -f install/env_model.yml
```

## Tokens
Open the file `etc/tokens.json.initial` and fill out your mesowest token. NOTE: the earthdata token is not needed for this paper

## RAWS Stash
This paper retrieved RAWS data from a stash maintained by Angel Farguell Caus at WIRC, please reach out to corresponding author Jonathon Hirschi if you'd like access. If you choose to use Synoptic data, you will need a paid token to access old data and results may not be exact to the paper

The file `etc/paths.yaml` will store your location to the RAWS stash. Update that file and run the test below and check for a positive print message:

```
conda activate ml_fmda_data
python tests/test_ingest_RAWS.py
```

## HRRR Retrieval

Run the following test to confirm HRRR data retrieval through the Herbie python package

```
python tests/test_ingest_HRRR.py
```

## Main project Config File
The configuration file `etc/rocky_config.yaml` contains the configuration instructions to repeat the analysis used in the paper. The relevant confirations are:
- `train_start`: set to first hour UTC of January 1st 2025. Used to define training period for ML models as well as for controlling the data retrieval process
- `train_end`: set to last hour of 2023 UTC, used to control training period for ML models.
- "f_start": when to start forecasting, set to first hour of 2024.
- "f_end": when to end forecasting, set to last hour of 2024. Also used to control data retrieval process
- "bbox": spatial bounding box, set to '[37,-111,46,-95]' to bracket all of Rocky Mountain GACC as determined by NIFC.
- `features_list`: list of features used as inputs to model, see directory `etc/variable_metadata` for more information on how these are constructed
- `baselines`: list of models used to compare forecast accuracy
- `data_dir`: location to save retrieved data used to construct training and testing sets
- `valid_path`: path to file that contains manual QC on FMC sensors for the rocky mountain region
- `climatology_file`: location of climatology that covers the study region and time period. The climatology is ran separately once so it can be reused for other applications, and it doesn't utilize a spatial holdout cross validation approach
- `n_reps`: number of replications with new random seeds of spatial sampling for test/train and random seeds for random weight initialization in RNN and XGBoost
- `space_test_frac`: percent of available RAWS to use for test set

# Data Retrieval Instructions
Data need to be retrieved from RAWS through Synopticpy and HRRR through Herbie. The config file described above controls this. To retrieve the data 

```
conda activate ml_fmda_data
python src/ingest/get_fmda_data.py etc/rocky_config.yaml
```

Alternatively, if you have access to a computing cluster that uses a job scheduler such as Slurm, you can submit this script using:

```
sbatch build_fmda_data.sh etc/rocky_config.yaml
```

NOTE: this process could take several days depending on your computational resources.

# Run Forecast Analysis

Run the following below with your own output directory. NOTE: as of Nov 1 2025, I haven't written how to reproduce this step without a job scheduler, it would require manual running of hundreds of replication runs.

```
sbatch forecast_analysis_controller.sh /PATH_TO_OUTPUT/ etc/rocky_config.yaml
```

# Additional Materials

The process above runs `src/forecast_eval.py` to general tables used in the paper. Additional tables are generated by running:

```
conda activate ml_fmda_models
python src/report_materials.py /PATH_TO_OUTPUT/
```

## Sensitivity Analysis

To evaluate relative predictor importance, we ran a series of sensitivity analysis. We removed invidual predictors and sets of predictors and reran the analysis. Edit the `features_list` object in the config file, for example a config file to test predictors without geographic features (lat, lon, elev) would have the list: 

```
...
features_list = ['Ed', 'Ew', 'rain', 'solar', 'wind', 'doy', 'hod']
...

```

Below are the configurations we ran with descriptions of the comparison in question:

The following comparisons were for groups of predictors (weather, geographic, temporal)
- Full model, inputs from weather+geographic+temporal: ['Ed', 'Ew', 'solar', 'wind', 'elev', 'lon', 'lat', 'rain', 'hod', 'doy']
- Weather only model: ['Ed', 'Ew', 'solar', 'wind', 'rain']
- Weather + temporal: ['Ed', 'Ew', 'solar', 'wind', 'rain', 'doy', 'hod']
- Weather + geo:      ['Ed', 'Ew', 'solar', 'wind', 'rain', 'elev', 'lon', 'lat']
- No weather model (temporal and geo features): ['elev', 'lon', 'lat', 'hod', 'doy'] 


The following python script reads results from all forecasts directories and combines into latex format tables (space separated, order doesn't matter the code extracts the features list and organizes)


```
python src/sensitivity forecasts/rocky24_paper forecasts/rocky24_noweather ...
```




